# Best Practices -- Issues in Inference

```{r best-setup, warning=FALSE, message=FALSE, echo=FALSE}
library(readxl)
library(janitor)
library(data.table)
library(emmeans)
library(here)
here <- here::here
data_path <- "data"
```

## t-tests and ANOVA
Welch, paired

## Power
### "Types" of Error
I, II, S, M

## multiple testing

**Multiple testing** is the practice of adjusting *p*-values (and less commonly confidence intervals) to account for the expected increase in the frequency of Type I error when there are multiple tests (typically Null Hypothesis Significance Tests). Multiple testing tends to arise in two types of situations:

1. Multiple pairwise contrasts among treatment levels (or combinations of levels) are estimated.
2. The effects of a treatment on multiple responses are estimated. This can arise if
    a. there are multiple ways of measuring the consequences of something -- for example, an injurious treatment on plant health might effect root biomass, shoot biomass, leaf number, leaf area, etc.
    b. one is exploring the consequences of an effect on many, many outcomes -- for example, the expression levels of 10,000 genes between normal and obese mice.

Despite the ubiquitous presence of multiple testing in elementary biostatistics textbooks, in the applied biology literature, and in journal guidelines, the practice of adjusting *p*-values for multiple tests is highly controversial among statisticians. My thoughts:

1. In situations like (1) above, I advocate that researchers **do not adjust p-values for multiple tests**. In general, its a best practice to only estimate contrasts for which you care about because of some *a priori* model of how the system works. If you compare all pairwise contrasts of an experiment with many treatment levels and/or combinations, expect to find some false discoveries.
2. In situations like (2a) above, I advocate that researchers **do not adjust p-values for multiple tests**.
3. In situations like (2b) above, adjusting for the **False Discovery Rate** is an interesting approach. But, recognize that tests with small *p*-values are *highly provisional* discoveries of a patterns only and not a discovery of the causal sequelae of the treatment. For that, one needs to do the hard work of designing experiments that rigorously probe a working, mechanistic model of the system.

Finally, recognize that anytime there are multiple tests, Type M errors will arise due to the vagaries of sampling. This means that in a rank-ordered list of the effects, those at the top have measured effects that are probably bigger than the true effect. An alternative to adjusted *p*-values is a **penalized regression** model that shrinks effects toward the mean effect.

### Some background

The logic of multiple testing goes something like this: the more tests that a researcher does, the higher the probability that a false positive (Type I error) will occur, therefore a researcher should should adjust *p*-values so that the Type I error over the set (or "family") of tests is 5%. This adjusted Type I error rate is the "family-wise error rate". 

If a researcher carries out multiple tests *of data in which the null hypothesis is true*, what is the probability of finding at least one Type I error? This is easy to compute. If the frequency of Type I error for a single test is $\alpha$, then the probability of no Type I error is $1 - \alpha$. For two tests, the probability of no Type I error in either test is the product of the probability for each test, or $(1 - \alpha)^2$. By the same logic, for $m$ tests, the probabilty of no type I error in any of the tests is $(1 - \alpha)^m$. The probability of at least one type one error, across the $m$ tests, then, is $1 - (1 - \alpha)^m$. A table of these probabilities for different $m$ is given below. If the null is true in all tests, then at least one Type I error is more likely than not if there are 14 tests, and close to certain if there more than 50 tests. Don't skip over this paragraph -- the logic is important even if I don't advocate adjusting for multiple tests.

```{r best-type1-table, echo=FALSE}
m <- c(1, 3, 6, 10, 50, 100)
p <- 1 - (1 - 0.05)^m
knitr::kable(data.table(m = m,
                        p = p),
             digits=c(0, 2),
             caption="Probability of at least one type I error within the set of multiple tests, for data in which the null hypothesis is true. The Type I error rate for a single test is 0.05. The number of tests is m. The probability is p.")
```

### Multiple testing -- working in R
#### Multiple pairwise contrasts

The `adjust` argument in `emmeans::contrast()` controls the method for *p*-value adjustment. The default is "tukey".

1. "none" -- no adjustment, in general my preference.
2. "tukey" -- Tukey's HSD, the default
3. "bonferroni" -- the standard bonferroni, which is conservative
4. "fdr" -- the false discovery rate
5. "mvt" -- based on the multivariate *t* distribution and using covariance structure of the variables

#### Tukey HSD adjustment of all pairwise comparisons

The data are those from Fig. 2D of "Data from The enteric nervous system promotes intestinal health by constraining microbiota composition". There is a single factor with four treatment levels. The response is neutrophil count.

```{r best-import-enteric, echo=FALSE}
folder <- "Data from The enteric nervous system promotes intestinal health by constraining microbiota composition"
filename <- "journal.pbio.2000689.s008.xlsx"

# figure 2D data
sheet_i <- "Figure 2"
range_i <- "F2:I24"
file_path <- here(data_path, folder, filename)
#file_path <- paste(data_path, folder, fn, sep="/")
dt_wide <- data.table(read_excel(file_path, sheet=sheet_i, range=range_i))
# clean names
dt_wide <- clean_names(dt_wide)
# get rid of "_donor"
setnames(dt_wide, old=colnames(dt_wide), new=gsub("_donor", "", colnames(dt_wide)))
# wide to long
exp2d <- na.omit(melt(dt_wide, measure.vars=colnames(dt_wide), variable.name="donor", value.name="count"))
exp2d[, donor:=factor(donor, c("wt", "gf", "sox10", "iap_mo"))]
```

No adjustment:

```{r}
m1 <- lm(count ~ donor, data=exp2d)
m1.emm <- emmeans(m1, specs="donor")
m1.pairs.none <- contrast(m1.emm, method="revpairwise", adjust="none")
summary(m1.pairs.none, infer=c(TRUE, TRUE))
```

Tukey HSD:

```{r}
m1.pairs.tukey <- contrast(m1.emm, method="revpairwise", adjust="tukey")
summary(m1.pairs.tukey, infer=c(TRUE, TRUE))
```

#### Tukey HSD adjustment of conditional effects
Here I compare the Tukey HSD adjustment between all pairwise comparisons and the subset that are the conditional (simple) effects. The data are those from Fig. 2D of "Data from The enteric nervous system promotes intestinal health by constraining microbiota composition". There is a single factor with four treatment levels. The response is neutrophil count.

## p-hacking

## difference in p is not different

## transformation, non-parametric, not "pre-testing"

## max vs. mean

## pre-post, normalization
