# Models with random factors -- Blocking and pseudoreplication {#lmm}

## Most experiments in bench biology generate batched data

```{r lmm-setup, echo=FALSE, message=FALSE, warning=FALSE}

library(here)
library(janitor)
library(readxl)
library(data.table)

# analysis packages
library(emmeans)
library(car) # qqplot, spreadlevel
library(afex)
library(lmerTest)
library(nlme)

# graphing an tabling packages
library(ggplot2) # ggplot environment
library(ggpubr) # publication ready plots
library(cowplot) # combine plots
library(knitr)
library(kableExtra) #tables
library(equatiomatic)

ggplot_the_model_path <- here::here("R/ggplot_the_model.R")
source(ggplot_the_model_path)

here <- here::here
clean_names <- janitor::clean_names
data_folder <- "data"
minus <- "\u2013"

here <- here::here
data_path <- "data"
```

```{r lmm-fig-sizes, echo=FALSE}
dpi <- 72
# width of bookdown page is 800 pix
# width of standard bookdown fig is 560 pix or 70% of page
std_width <- 504/dpi # 7 in
full_width <- 800/dpi
small_scale = 6/7
small_width <- std_width*small_scale # 6 in

# standard aspect ratio is .7 so
std_ar <- 5/7 # .71
response_ar <- .8 # for use with response plots with p-values
effect_ar <- 0.6 # for effects
harrell_ar <- 1 # for harrell effect & response plots

# dims (width, height)
std_dim <- c(std_width, std_width*std_ar)
response_dim <- c(std_width, std_width*response_ar)
effect_dim <- c(std_width, std_width*effect_ar)
harrell_dim <- c(std_width, std_width*harrell_ar)

# out.width percents
out.width_std <- paste0(std_width/full_width*100, "%")
out.width_small <- paste0(small_width/full_width*100, "%")

```

Most data in experimental biology is composed of subsets -- or **batches** of data that result in **correlated error** that contaminates statistical inference unless the error is modeled.
In some designs, modeling the correlated error increases precision and power, and ultimately decreases false discovery. In other designs, failure to model the correlated error results in incorrectly high precision and incorrectly low *p*-values, leading to increased rates of false discovery.

What do I mean by "batch" and how can correlated error both increase and decrease false discovery? Consider these experiments:

1. The experiment in Figure \@ref(fig:lmm-biological-replicates) is a factorial design with two factors, $\texttt{genotype}$ and $\texttt{treatment}$, each with two levels. One mice of each treatment combination is randomly assigned to a cage. There are five replicates of each cage. The five replicate mice per treatment combination are **biological replicates**. Each cage is a batch. Each cage has a unique set of factors that contribute to the error variance of the measure of the response. Each cage shares a cage-specific history of temperature, humidity, food, light, interactions with animal facilities staff and behavioral interactions among the mice. All response measures within a cage share the component of the error variance unique to that cage and, as a consequence, the error (residuals) within a cage are more similar to each other than they are to the residuals between cages.

```{r lmm-biological-replicates, echo=FALSE}
path1 <- here("images", "lmm-biological-replicates-435-273.png")
include_graphics(path1)
```

2. The experiment in Figure \@ref(fig:lmm-technical-replicates) is a factorial design with two factors, $\texttt{genotype}$ and $\texttt{treatment}$, each with two levels. One mice of each treatment combination is randomly assigned to a cage. There are five replicates of each cage. The researchers take three measures of the response variable per mouse. The three measures are **subsampled replicates** (most often referred to as **subsamples**). The subsamples could be **technical replicates** if multiple measures are taken from the same prep or **repeated measures** if the the multiple measures are taken at different time points. In addition to each cage being a batch, each mouse is a batch. Each mouse has a unique set of factors that contribute to the error variance of the measures of the response in that mouse. All response measures within a mouse share the component of the error variance unique to that mouse and, as a consequence, the error (residuals) within a mouse are more similar to each other than they are to the residuals between mice

```{r lmm-technical-replicates, echo=FALSE}
path2 <- here("images", "lmm-technical-replicates-435-273.png")
include_graphics(path2)
```

3. The experiment in Figure \@ref(fig:lmm-segregated) has a single factor $\texttt{treatment}$ with two levels. Importantly, the treatment (example: diet) is randomly assigned to cage and all mice in the cage have this treatment. The five mice per cage are **subsampled replicates** (**subsamples**). As in experiments 1 and 2, each cage is a batch. Each cage has a unique set of factors that contribute to the error variance of the measures of the response in that cage. All response measures within a cage share the component of the error variance unique to that cage and, as a consequence, the error (residuals) within a cage are more similar to each other than they are to the residuals among cages.

```{r lmm-segregated, echo=FALSE}
path3 <- here("images", "lmm-pseudoreplication.png")
include_graphics(path3)
```

In each of these experiments, there is systematic variation at multiple levels: among treatments due to treatment effects and among batches due to **batch effects**. Batches come in lots of flavors, including experiment, cage, flask, plate, slide, donor, and individual. The among-batch variation is the **random effect**. An assumption of modeling random effects is that the batches are a random sample of the batches that could have been sampled. This is often not strictly true as batches are often **convenience samples** (example: the human donors of the Type 2 diabetes beta cells are those that were in the hospital).

The variation among batches/lack of independence within batches has different consequences on the uncertainty of the estimate of a treatment effect. The batches in Experiment 1 contain all treatment combinations. The researcher is interested in the treatment effect but not the variation due to differences among the batches. The batches are nuissance factors that add additional variance to the response, with the consequence that estimates of treatment effects are less precise, unless the variance due to the batches is explicitly modeled. **Modeling a batch that contains some or all treatment combinations will increase precision and power**.

Batches that contain at least two treatment combinations are known as **blocks**. A block that contains all treatment combinations is a **complete block**. A block that contains less than all combinations is an **incomplete block**. Including block structure in the design is known as **blocking**. Adding a blocking factor to a statistical model is used to increase the precision of an estimated treatment effect. Experiment 1 is an example of a **randomized complete block** design. "Complete" means that each block has all treatment levels or combinations of levels if there is more than one factor.

In Experiment 2, there are multiple measures per mouse and the design is a **randomized complete block with subsampling**. The subsampling is not the kind of replication that can be used to infer the among treatment effect because the treatment assignment was not at the level of the subsamples. The **treatment replicates** (the cage) are the blocks, because *it was at this level that treatment assignment was randomized*. Nevertheless, there are inference advantages to subsampling. A statistical analysis of all measures from a subsampled design without modeling the correlated error due to the subsampling is a kind of [pseudoreplication](https://en.wikipedia.org/wiki/Pseudoreplication){target="_blank"}. Pseudoreplication results in incorrectly small standard errors and *p*-values and increased rates of false discovery.

In Experiment 3, the treatment is randomized *to* batch, so each batch contains only a single treatment level. In these **segregated** experimental designs, the variation among batches that arises from non-treatment related differences among batches **confounds** the variation among batches due to a true treatment effect. An extreme example of this would be an experiment with only a single cage with control conditions and a single cage with treatment conditions. Imagine 1) the true effect of the treatment is zero and 2) an aggressive mouse in the control cage stimulates the stress response in the other mice and this stress response has a large effect on the value of the response variable measured by the researchers. The researcher is fooled into thinking that the treatment caused the difference in the response. Again, mice are subsampled replicates while treatment replicates are at the level of the cage, because it was at this level that treatment assignment was randomized. This means the researcher has a single, treatment replicate (or, $n=1$), regardless of the number of mice (subsamples) in each cage. A statistical analysis of all measures from a subsampled design without modeling the correlated error due to the subsampling is a kind of [pseudoreplication](https://en.wikipedia.org/wiki/Pseudoreplication){target="_blank"}. Pseudoreplication results in incorrectly small standard errors and *p*-values and increased rates of false discovery.


```{r lmm-import-exp1g, echo=FALSE, message=FALSE}
data_from <- "A GPR174â€“CCL21 module imparts sexual dimorphism to humoral immunity"
file_name <- "41586_2019_1873_MOESM3_ESM.xlsx"
file_path <- here(data_folder, data_from, file_name)

exp1g_wide <- read_excel(file_path,
                         sheet = "Fig 1g",
                         range = "B4:E25",
                         col_types = c("numeric"),
                         col_names = FALSE) %>%
  data.table()

genotype_levels <- c("Gpr174+", "Gpr174-")
sex_levels <- c("M", "F")
g.by.s_levels <- do.call(paste, expand.grid(genotype_levels, sex_levels))
colnames(exp1g_wide) <- g.by.s_levels

exp_levels <- paste0("exp_", 1:4)
exp1g_wide[, experiment_id := rep(exp_levels, c(5,6,6,5))] #check!
exp1g_wide[, experiment_id := factor(experiment_id)] #check!

exp1g <- melt(exp1g_wide,
              id.vars = "experiment_id",
              measure.vars = g.by.s_levels,
              variable.name = "treatment",
              value.name = "gc") %>% # cell count
  na.omit()

exp1g[, c("genotype", "sex"):= tstrsplit(treatment,
                                             " ",
                                             fixed = TRUE)]
exp1g[, genotype := factor(genotype,
                           levels = genotype_levels)]
exp1g[, sex := factor(sex,
                           levels = sex_levels)]

exp1g_means <- exp1g[, .(gc = mean(gc)),
                     by = .(treatment, genotype, sex, experiment_id)]

```

## Why we care about linear mixed models

The experiment that will be introduced in Example 1 was replicated four times. Looking at Figure \@ref(fig:lmm-why-care) shows why we shouldn't think of each independent experiment as a random sample *from the same hat*. Within any treatment level, the data from each experiment is **clustered**. Clearly, there is something systematically different among the experiments that is affecting the results. Which results should we report? Or, do we combine the experiments?

While frequently done, do not simply report one of the results and claim "similar results in the other three independent expeiments". Some aspects of the four experiments are qualitatively similar, for example, the direction of the estimated effect of genotype is positive in males and negative in females in all four experiments. But the evidence against the null for this pattern varies among experiments -- the *p*-values for a genotype effect in males and females are 0.1 and 0.047 in experiment 1, 0.08 and 0.16 in experiment 2, 0.013 and 0.11, and in experiment 3, and 0.12 and 0.006 in experiment 4. There will be a strong urge to choose to report the experiment that is closest to the goal of the researcher. Plus, why would we not report data that we bothered to collect and analyze?

Researchers typically combine the experiment in one of two ways

1. **complete pooling** by simply ignoring $\texttt{experiment_id}$. That is, just combine all the data and fit the model. The *p*-values for a genotype effect in males and females using the model with complete pooling are 0.032 and 0.028. Inference (confidence intervals and *p*-values) from complete pooling assumes no correlated error. We know from the conspicuous by-experiment clustering in Figure \@ref(fig:lmm-why-care) that there is correlated error in these data. The individual values within each experiment are pseudoreplicates. The consequence of the correlated error in complete pooling is anti-conservative inference -- incorrectly small standard errors and *p*-values.
2. means pooling by computing the means of each treatment for each experiment and then analyzing the means. The *p*-values for a genotype effect in males and females using the model with means pooling are 0.29 and 0.025. Inference (confidence intervals and *p*-values) from means pooling assumes no correlated error. Analyzing the means does not correct for the clustering -- it is easy to see from Figure \@ref(fig:lmm-why-care) that the mean of exp_4 will be high in all four treatment groups. The consequence of the correlated error in means pooling will usually by conservative inference -- incorrectly large standard errors and *p*-values, with a subsequent loss of power.

To analyze these data with a model that results in inferential statistics that mean what they say (confidence intervals that really reflect 95% coverage and *p*-values that really reflect 5% Type I error when the null is true) we need to model the correlated error. There are several ways to do this. Here we use a **linear mixed model**.

```{r, echo=FALSE}
fit <- lm(gc ~ genotype * sex, data = exp1g[experiment_id == "exp_1"])
p1 <- (emmeans(fit, specs = c("genotype", "sex")) %>%
  contrast(method = "revpairwise", adjust = "none") %>%
  summary)[c(1,6), "p.value"]

fit <- lm(gc ~ genotype * sex, data = exp1g[experiment_id == "exp_2"])
p2 <- (emmeans(fit, specs = c("genotype", "sex")) %>%
  contrast(method = "revpairwise", adjust = "none") %>%
  summary)[c(1,6), "p.value"]

fit <- lm(gc ~ genotype * sex, data = exp1g[experiment_id == "exp_3"])
p3 <- (emmeans(fit, specs = c("genotype", "sex")) %>%
  contrast(method = "revpairwise", adjust = "none") %>%
  summary)[c(1,6), "p.value"]

fit <- lm(gc ~ genotype * sex, data = exp1g[experiment_id == "exp_4"])
p4 <- (emmeans(fit, specs = c("genotype", "sex")) %>%
  contrast(method = "revpairwise", adjust = "none") %>%
  summary)[c(1,6), "p.value"]

fit <- lm(gc ~ genotype * sex, data = exp1g)
p_complete <- (emmeans(fit, specs = c("genotype", "sex")) %>%
  contrast(method = "revpairwise", adjust = "none") %>%
  summary)[c(1,6), "p.value"]

fit <- lm(gc ~ genotype * sex,
          data = exp1g[, .(gc = mean(gc)),
                       by = .(genotype, sex, experiment_id)])
p_means <- (emmeans(fit, specs = c("genotype", "sex")) %>%
  contrast(method = "revpairwise", adjust = "none") %>%
  summary)[c(1,6), "p.value"]



```

```{r lmm-why-care, fig.cap = "The experiment introduced in Example 1 was replicated four times."}
exp1g_means <- exp1g[, .(gc = mean(gc)),
                        by = .(treatment, experiment_id)]
pd_width <- 0.4
gg <- ggplot(data = exp1g,
       aes(x = treatment,
           y = gc,
           color = experiment_id)) +

  geom_point(alpha = 0.5,
             position = position_dodge(pd_width)) +
  scale_color_manual(values = pal_okabe_ito_blue) +
  
  geom_point(data = exp1g_means,
            aes(x = treatment,
                y = gc),
            size = 3,
            position = position_dodge(pd_width)) +
  
  geom_line(data = exp1g_means,
            aes(x = treatment,
                y = gc,
                group = experiment_id),
            position = position_dodge(pd_width)) +
  
  theme_pubr() +
  theme(axis.title.x = element_blank()) +
  
  NULL

gg

```

## Example 1 -- Using independent experiments as blocks to increase precision and power ("germinal centers" -- Experiment 1g)

### Understand the data

### Examine the data

```{r lmm-exp1g-examine}
ggplot(data = exp1g,
       aes(x = treatment,
           y = gc,
           color = experiment_id)) +
  geom_point(position = position_dodge(0.4))

```

### Fit the model

```{r}
exp1g_m1 <- lmer(gc ~ treatment + (treatment | experiment_id),
                 data = exp1g)
exp1g_m1 <- lmer(gc ~ genotype * sex +
                   (treatment | experiment_id),
                 data = exp1g)
exp1g_m1 <- lmer(gc ~ genotype * sex +
                   (genotype * sex | experiment_id),
                 data = exp1g)

```

Notes

1. I have flattened the factorial design in this analysis for the purpose of the explainer. There is no interaction effect in the model, but we can still get this from the contrasts.

### Inference from the model

```{r}
exp1g_m1_emm <- emmeans(exp1g_m1, specs = c("genotype", "sex"))
# m1_emm
```


```{r}
# m1_emm # print in console to get row numbers
# set the mean as the row number from the emmeans table
wt_m <- c(1,0,0,0)
ko_m <- c(0,1,0,0)
wt_f <- c(0,0,1,0)
ko_f <- c(0,0,0,1)

# simple effects within males and females + interaction 
# 1. (ko_m - wt_m) 
# 2. (ko_f - wt_f)

exp1g_m1_planned <- contrast(exp1g_m1_emm,
                       method = list(
                         "KO/M - WT/M" = c(ko_m - wt_m),
                         "KO/F - WT/F" = c(ko_f - wt_f),
                         "Interaction" = c(ko_f - wt_f) -
                           c(ko_m - wt_m)
                           
                       ),
                       adjust = "none"
) %>%
  summary(infer = TRUE)

exp1g_m1_planned %>%
  kable(digits = c(1,2,3,3,2,2,2,3)) %>%
  kable_styling()
```

### Plot the model

## Understanding linear mixed models 1 --
### Linear mixed models have parameters for random intercept effects and random slope effects

The linear mixed model fit to `exp1g` data is

$$
\begin{align}
\texttt{gc}_j = \ &\beta_0 + \beta_1 (\texttt{genotype}_\texttt{KO}) + \beta_2 (\texttt{sex}_\texttt{F}) + \beta_3 (\texttt{genotype}_\texttt{KO}:\texttt{sex}_\texttt{F}) \ + \\
&\gamma_{0j} + \gamma_{1j}(\texttt{genotype}_\texttt{KO}:\texttt{sex}_\texttt{M}) +  \gamma_{2j}(\texttt{genotype}_\texttt{WT}:\texttt{sex}_\texttt{F}) \ + \\  &\gamma_{3j}(\texttt{genotype}_\texttt{KO}:\texttt{sex}_\texttt{F}) + \varepsilon
\end{align}
$$
The $\beta$ are the **fixed effects**, although $\beta_$ is not an effect but a mean. The $\gamma$ (the lower case, greek letter "gamma") are the **random effects**. All of these, including the $\gamma_0$, are effects (differences between means). The *j* in the $\gamma$ references the *j*th experiment -- this means that there is a different $\gamma_0$, $\gamma_1$, $\gamma_2$, and $\gamma_3$ for each experiment. In contrast, $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$ are the same for all experiments -- this is why they are "fixed". The $\gamma$ are modeled as if each experiment is a random draw from an infinite number of experiments. This is why the $\gamma$ are "random".

A reminder of what these fixed effects are is illustrated in Figure \@ref(fig:lmm-eg1-explainer_1). [A more thorough explanation is in the chapter Linear models with two categorical $X$ -- Factorial linear models ("two-way ANOVA")](#factorial)

A **random intercept** for experiment *j* is the sum of the fixed intercept ($\beta_0$) and a **random intercept effect** $\gamma_0j$ (Figure \@ref(fig:lmm-eg1-explainer-2)).

A **random slope** is illustrated in Figure \@ref(fig:lmm-eg1-explainer-3) for the treatment combination "GPR174+ F". The random slope for experiment *j* is the sum of the fixed slope ($\beta_2$) and a **random slope effect** $\gamma_2j$. This makes $\gamma_2j$ equal to the residual of the modeled mean of the "GPR174+ F" treatment for experiment *j* and the expected mean if there were no random slope. This expected mean is the fixed intercept plus the fixed effect for the "GPR174+ F" treatment plus the random intercept effect for experiment *j*, or $\beta_0 + \beta_1 + \gamma_{0j}$. These expected means (if each $\gamma_{0j}=0$) are illustrated by the large gray dots in Figure \@ref(fig:lmm-eg1-explainer-3). The random slopes can also be visualized by the lines connecting the modeled means for each experiment from the reference to the "GPR174+ F" treatment (Figure \@ref(fig:lmm-eg1-explainer-3)).

### A reminder of what fixed effects are

```{r lmm-eg1-explainer_1, echo= FALSE, fig.cap = "Fixed effects estimated by exp1g_m1. The light gray point is the expected value of the GPR174- F treatment if genotype and sex were additive."}
exp1g_emm_dt <- summary(exp1g_m1_emm) %>%
  data.table()
treatment_levels <- levels(exp1g$treatment) %>%
  as.character
exp1g_emm_dt[, treatment := factor(treatment_levels,
                                   levels = treatment_levels)]

b <- coef(summary(exp1g_m1))[, "Estimate"]

pd_width <- 0.4
gg <- ggplot(data = exp1g,
       aes(x = treatment,
           y = gc,
           color = experiment_id)) +
  
  geom_point(alpha = 0.5,
             position = position_dodge(pd_width)) +
  scale_color_manual(values = pal_okabe_ito_blue) +
  
  geom_point(data = exp1g_emm_dt,
             aes(x = treatment,
                 y = emmean),
             color = "black",
             size = 3) +
  
  geom_segment(x = 0.75,
               y = b[1],
               xend = 4.25,
               yend = b[1],
               linetype = "dashed",
               color = "gray30") +
  
  annotate(geom = "text",
           x = .7,
           y = b[1],
           hjust = 1,
           label = "b[0]",
           parse = TRUE) +
  
  geom_bracket(
    x = 2.1,
    y = b[1],
    yend = b[1] + b[2],
    label = "b[1]",
    text.size = 4,
    text.hjust = 0,
    color = "black",
    parse = TRUE) +
  
  geom_bracket(
    x = 3.1,
    y = b[1],
    yend = b[1] + b[3],
    label = "b[2]",
    text.size = 4,
    text.hjust = 0,
    color = "black",
    parse = TRUE) +
  
  geom_point(aes(x = 4,
                 y = b[1] + b[2] + b[3]),
             color = "gray",
             alpha = 0.5,
             size = 3) +

  geom_bracket(
    x = 4.1,
    y = b[1] + b[2] + b[3],
    yend = b[1] + b[2] + b[3] + b[4],
    label = "b[3]",
    text.size = 4,
    text.hjust = 0,
    color = "black",
    parse = TRUE) +
  
  
  theme_pubr() +
  theme(axis.title.x = element_blank()) +
  
  NULL
gg
```

### What are random intercepts?

```{r lmm-eg1-explainer-2, echo=FALSE, fig.cap="What random intercepts are. The large, colored dots are the modeled mean for each treatment (genotype by sex) by experiment_id combination. The dashed grey line is the fixed intercept -- it is the modeled mean of the reference level (\"GPR174+ M\"). The large, colored dots at the reference level are the random intercepts. The value of each random intercept is the sum of the fixed intercept and the random intercept effect for that experiment_id. Each random intercept effect is the residual from (or, distance from colored dot to) the dashed, gray line, shown by the vertical, colored lines"}
exp1g[, m1_fit := predict(exp1g_m1)]
exp1g_gr_means <- exp1g[, .(emmean = mean(m1_fit)),
                        by = .(treatment, experiment_id)]
b <- coef(summary(exp1g_m1))[, "Estimate"]
u <- ranef(exp1g_m1)$experiment_id

experiment_id <- levels(exp1g$experiment_id)
pd_width <- 0.4

gg1 <- ggplot(data = exp1g,
       aes(x = treatment,
           y = gc,
           color = experiment_id)) +
  scale_color_manual(values = pal_okabe_ito_blue) +
  geom_point(alpha = 0.5,
             position = position_dodge(pd_width)) +

  # b_0
  geom_segment(x = 1 - pd_width/2,
               y = b[1],
               xend = 1 + pd_width/2,
               yend = b[1],
               linetype = "dashed",
               color = "gray") +
  
  # treatment x experiment id points
  geom_point(data = exp1g_gr_means,
            aes(x = treatment,
                y = emmean),
            size = 3,
            position = position_dodge(pd_width)) +

  # treatment x experiment id lines for intercept
  geom_segment(data = exp1g_gr_means[1:4],
               aes(x = c(1 - pd_width/2.6,
                         1 - pd_width/8,
                         1 + pd_width/8,
                         1 + pd_width/2.6),
                   y = rep(b[1], 4),
                   xend = c(1 - pd_width/2.6,
                            1 - pd_width/8,
                            1 + pd_width/8,
                            1 + pd_width/2.6),
                   yend = emmean,
                   color = experiment_id)) +

  
  geom_bracket(
    x = 1 - pd_width/1.7,
    y = b[1] + min(u[,1]),
    yend = b[1] + max(u[,1]),
    tip.length = -0.01,
    label = "gamma[0][j]",
    text.size = 4,
    text.nudge_x = -0.02,
    text.hjust = 1,
    color = "black",
    parse = TRUE) +

  theme_pubr() +
  theme(axis.title.x = element_blank()) +
  
  NULL

gg1
```

### What are random slopes?

```{r lmm-eg1-explainer-3, echo=FALSE, fig.cap="What random slopes are. The large, black dots are the modeled means for the reference (\"GPR174+ M\") and \"GPR174+ F\" treatment combinations. The difference in the y-value of these two points is the fixed effect $b_2$, which is the effect of \"GPR174+ F\" relative to the reference. This effect can be visualized as the slope of the black line -- it is a fixed slope. The large, gray dots are the expected modeled means of \"GPR174+ F\" treatment combination for each experiment_id if there were no random slope. The differences between each large, colored dots and large, gray dot is the random slope effect for that experiment_id for the \"GPR174+ F\" treatment combination. The value of these effects for each experiment_id is equivalent to the difference of each of the colored lines (the random slopes) and the black line (the fixed slope)."}

b <- coef(summary(exp1g_m1))[, "Estimate"]
u <- ranef(exp1g_m1)$experiment_id
experiment_id <- levels(exp1g$experiment_id)
female_pos <- exp1g_gr_means[treatment == levels(treatment)[3]]
female_pos[, zero_slope_mean := b[1] + b[3] + u[,1]]

pd_width <- 0.4

gg2 <- ggplot(data = exp1g,
       aes(x = treatment,
           y = gc,
           color = experiment_id)) +
  scale_color_manual(values = pal_okabe_ito_blue) +
  geom_point(alpha = 0.5,
             position = position_dodge(pd_width)) +
  
  # fixed b_2 slope
  geom_point(data = exp1g_emm_dt[c(1,3)],
             aes(x = treatment,
                 y = emmean),
             size = 3,
             color = "black") +
  geom_segment(x = 1,
               y = exp1g_emm_dt[1, emmean],
               xend = 3,
               yend = exp1g_emm_dt[3, emmean],
               color = "black") +

  # expected means if gamma_2 = 0
  geom_point(data = female_pos,
             aes(x = treatment,
                 y = zero_slope_mean,
                 group = experiment_id),
             size = 3,
             color = "gray",
             position = position_dodge(width = pd_width)) +

  # random b_2 slopes
  geom_segment(data = data.table(
    experiment_id = experiment_id,
    x = c(1 - pd_width/2.5,
          1 - pd_width/5,
          1 + pd_width/5,
          1 + pd_width/2.5),
    y = c(b[1] + u[1,1],
          b[1] + u[2,1],
          b[1] + u[3,1],
          b[1] + u[4,1]),
    xend = c(3 - pd_width/2.5,
             3 - pd_width/5,
             3 + pd_width/5,
             3 + pd_width/2.5),
    yend = c(b[1] + b[3] + u[1,1] + u[1,3],
             b[1] + b[3] + u[2,1] + u[2,3],
             b[1] + b[3] + u[3,1] + u[3,3],
             b[1] + b[3] + u[4,1] + u[4,3])),
    aes(x = x,
        y = y,
        xend = xend,
        yend = yend)) +
  
  geom_bracket(
    x = 3 + pd_width/1.7,
    y = b[1] + b[3] + min(u[,1] + u[,3]),
    yend = b[1] + b[3] + max(u[,1] + u[,3]),
    label = "gamma[2]^2",
    text.size = 4,
    text.hjust = 0,
    color = "black",
    parse = TRUE) +

  geom_point(data = exp1g_gr_means,
            aes(x = treatment,
                y = emmean),
            size = 3,
            position = position_dodge(pd_width)) +

  theme_pubr() +
  theme(axis.title.x = element_blank()) +
  
  NULL

gg2
```


## Example 2 -- Using a blocked design to increase precision and power (no subsampling) -- paired t

## Example 3 - pseudoreplication

