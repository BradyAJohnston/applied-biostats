# Best Practices -- Issues in Inference

```{r best-setup, warning=FALSE, message=FALSE, echo=FALSE}
library(readxl)
library(janitor)
library(data.table)
library(MASS)
library(emmeans)
library(ggpubr)
library(here)
library(lmPerm, quietly=T)
here <- here::here
data_path <- "data"
```

## t-tests and ANOVA
Welch, paired

## Power
### "Types" of Error
I, II, S, M

## multiple testing

**Multiple testing** is the practice of adjusting *p*-values (and less commonly confidence intervals) to account for the expected increase in the frequency of Type I error when there are multiple tests (typically Null Hypothesis Significance Tests). Multiple testing tends to arise in two types of situations:

1. Multiple pairwise contrasts among treatment levels (or combinations of levels) are estimated.
2. The effects of a treatment on multiple responses are estimated. This can arise if
    a. there are multiple ways of measuring the consequences of something -- for example, an injurious treatment on plant health might effect root biomass, shoot biomass, leaf number, leaf area, etc.
    b. one is exploring the consequences of an effect on many, many outcomes -- for example, the expression levels of 10,000 genes between normal and obese mice.

Despite the ubiquitous presence of multiple testing in elementary biostatistics textbooks, in the applied biology literature, and in journal guidelines, the practice of adjusting *p*-values for multiple tests is highly controversial among statisticians. My thoughts:

1. In situations like (1) above, I advocate that researchers **do not adjust p-values for multiple tests**. In general, its a best practice to only estimate contrasts for which you care about because of some *a priori* model of how the system works. If you compare all pairwise contrasts of an experiment with many treatment levels and/or combinations, expect to find some false discoveries.
2. In situations like (2a) above, I advocate that researchers **do not adjust p-values for multiple tests**.
3. In situations like (2b) above, adjusting for the **False Discovery Rate** is an interesting approach. But, recognize that tests with small *p*-values are *highly provisional* discoveries of a patterns only and not a discovery of the causal sequelae of the treatment. For that, one needs to do the hard work of designing experiments that rigorously probe a working, mechanistic model of the system.

Finally, recognize that anytime there are multiple tests, Type M errors will arise due to the vagaries of sampling. This means that in a rank-ordered list of the effects, those at the top have measured effects that are probably bigger than the true effect. An alternative to adjusted *p*-values is a **penalized regression** model that shrinks effects toward the mean effect.

### Some background

The logic of multiple testing goes something like this: the more tests that a researcher does, the higher the probability that a false positive (Type I error) will occur, therefore a researcher should should adjust *p*-values so that the Type I error over the set (or "family") of tests is 5%. This adjusted Type I error rate is the "family-wise error rate". 

If a researcher carries out multiple tests *of data in which the null hypothesis is true*, what is the probability of finding at least one Type I error? This is easy to compute. If the frequency of Type I error for a single test is $\alpha$, then the probability of no Type I error is $1 - \alpha$. For two tests, the probability of no Type I error in either test is the product of the probability for each test, or $(1 - \alpha)^2$. By the same logic, for $m$ tests, the probabilty of no type I error in any of the tests is $(1 - \alpha)^m$. The probability of at least one type one error, across the $m$ tests, then, is $1 - (1 - \alpha)^m$. A table of these probabilities for different $m$ is given below. If the null is true in all tests, then at least one Type I error is more likely than not if there are 14 tests, and close to certain if there more than 50 tests. Don't skip over this paragraph -- the logic is important even if I don't advocate adjusting for multiple tests.

```{r best-type1-table, echo=FALSE}
m <- c(1, 3, 6, 10, 50, 100)
p <- 1 - (1 - 0.05)^m
knitr::kable(data.table(m = m,
                        p = p),
             digits=c(0, 2),
             caption="Probability of at least one type I error within the set of multiple tests, for data in which the null hypothesis is true. The Type I error rate for a single test is 0.05. The number of tests is m. The probability is p.")
```

### Multiple testing -- working in R
#### Multiple pairwise contrasts

The `adjust` argument in `emmeans::contrast()` controls the method for *p*-value adjustment. The default is "tukey".

1. "none" -- no adjustment, in general my preference.
2. "tukey" -- Tukey's HSD, the default
3. "bonferroni" -- the standard bonferroni, which is conservative
4. "fdr" -- the false discovery rate
5. "mvt" -- based on the multivariate *t* distribution and using covariance structure of the variables

#### Tukey HSD adjustment of all pairwise comparisons

The data are those from Fig. 2D of "Data from The enteric nervous system promotes intestinal health by constraining microbiota composition". There is a single factor with four treatment levels. The response is neutrophil count.

```{r best-import-enteric, echo=FALSE}
folder <- "Data from The enteric nervous system promotes intestinal health by constraining microbiota composition"
filename <- "journal.pbio.2000689.s008.xlsx"

# figure 2D data
sheet_i <- "Figure 2"
range_i <- "F2:I24"
file_path <- here(data_path, folder, filename)
#file_path <- paste(data_path, folder, fn, sep="/")
dt_wide <- data.table(read_excel(file_path, sheet=sheet_i, range=range_i))
# clean names
dt_wide <- clean_names(dt_wide)
# get rid of "_donor"
setnames(dt_wide, old=colnames(dt_wide), new=gsub("_donor", "", colnames(dt_wide)))
# wide to long
exp2d <- na.omit(melt(dt_wide, measure.vars=colnames(dt_wide), variable.name="donor", value.name="count"))
exp2d[, donor:=factor(donor, c("wt", "gf", "sox10", "iap_mo"))]
```

No adjustment:

```{r}
m1 <- lm(count ~ donor, data=exp2d)
m1.emm <- emmeans(m1, specs="donor")
m1.pairs.none <- contrast(m1.emm, method="revpairwise", adjust="none")
summary(m1.pairs.none, infer=c(TRUE, TRUE))
```

Tukey HSD:

```{r}
m1.pairs.tukey <- contrast(m1.emm, method="revpairwise", adjust="tukey")
summary(m1.pairs.tukey, infer=c(TRUE, TRUE))
```

#### Tukey HSD adjustment of conditional effects
Here I compare the Tukey HSD adjustment between all pairwise comparisons and the subset that are the conditional (simple) effects.

## p-hacking

## difference in p is not different

## Inference when data are not Normal

Inference in statistical models (standard errors, confidence intervals, *p*-values) are a function of the modeled distributions of the parameters (for linear models, this parameter is the conditional (or error) variance $\sigma^2$); if the data do not approximate the modeled distribution, then inferential statistics might be to liberal (standard errors are too small, confidence intervals are too narrow, Type I error is more than nominal) or to conservative (standard errors are too large, confidence intervals are too wide, Type I error is less than nominal).

Linear models assume that the "the data" (specifically, the conditional values, or, equivalently, the residuals from the model) approximate a Normal distribution. Chapter xxx showed how to qualitatively assess how well residuals approximate a Normal distribution using a Q-Q plot. If the researcher concludes that the data poorly approximate a normal distribution because of outliers, the researcher can use robust methods to estimate the parameters. If the approximation is poor because the residuals suggest a skewed distribution or one with heavy or light tails, the researcher cancan choose among several strategies

1. continue to use the linear model, many of which are fairly robust to non-normal data, especially when the sample size is not small.
2. transform the data in a way that makes the conditional response more closely approximate a normal distribution.
3. use a non-parametric method, which are methods that do not assume a particular distribution
4. use a generalized linear model (GLM), which is appropriate if the conditional response approximates any of the distributions that can be modeled using GLM.

Do not use the *p*-value from a "test for normality" (such as a Shapiro-Wilk test) to decide between using the linear model (or t-test or ANOVA) and an alternative such as a generalized linear model (or transformation or non-parametric test). No real data is normal. Tests of normality will tend to "not reject" normality (p > 0.05) when the sample size is small and "reject" normality (p < 0.05) when the sample size is very large. But again, a "not rejected" hypothesis test does not mean the null (in this case, the data are normal) is true. More importantly, where the test for normality tends to fail to reject (encouraging a researcher to use parametric statistics) is where parametric inference performs the worst (because of small *n*) and where the test for normality tends to reject (encouraging a researcher to use non-parametric statistics) is where the parametric inference performs the best (because of large sample size) (Lumley xxx).

### Log transformations

Many response variables within biology, including count data, and almost anything that grows, are right skewed and have variances that increase with the mean. A log transform of a response variable with this kind of distribution will tend to make the residuals more approximately normal and the variance less dependent of the mean. At least two issues arise

1. if the response is count data, and the data include counts of zero, then a fudge factor has to be added to the response since log(0) doesn't exist. The typical fudge factor is to add 1 to *all* values, but this is arbitrary and results do depend on the magnitude of this fudge factor.
2. the estimates are on a log scale and do not have the units of the response. The estimates can be back-transformed by taking the exponent of a coefficient or contrast but this itself produces problems. For example, the backtransformed mean of the log-transformed response is not the mean on the origianl scale (the arithmetic mean) but the **geometric mean**. Geometric means are smaller than arithmetic means, appreciably so if the data are heavily skewed. Do we want our understanding of a system to be based on geometric means?

### Non-parametric tests

1. In general, the role of a non-parametric test is a better-behaved *p*-value, that is, one whose Type I error is well controlled. As such, non-parametric tests are more about Null-Hypothesis Statistical Testing and less (or not at all) about Estimation.
2. In general, classic non-parametric tests are only available for fairly simple experimental designs. Classic non-parametric tests include
    * Independent sample (Student's) *t* test: Mann-Whitney-Wilcoxan
    * Paired *t* test: Wilcoxan signed-rank test
    
One rarely sees non-parametric tests for more complex designs that include covariates, or multiple factors, but for these, one could 1) convert the response to ranks and fit the usual linear model, or 2) implement a permutation test that properly preserves **exchangeability**.

Permutation tests control Type I error and are powerful. That said, I would recommend a permutation test as a supplment to, and not replacement of, inference from a generalized linear model.

#### Permutation test

A permutation test effectively computes the probability that a random assignment of a response to a particular value of *X* generates a test statistic as large or larger than the observed statistic. If this probability is small, then this "random assignment" is unlikely. From this we infer that the actual assignment matters, which implies a treatment effect.

Create data from a negative binomial distribution, which is right skewed

```{r}
set.seed(5)
n <- 10
fake_data <- data.table(
  treatment = rep(c("cn", "tr"), each=n),
  count = rnegbin(n*2, mu=rep(c(10, 14), each=n), theta=1)
)
```

Some statistics of the fake data

```{r}
fake_data[, .(N=.N, mean=mean(count), sd=sd(count)), by=treatment]
```

A plot of the fake data

```{r}
ggerrorplot(x="treatment",
            y = "count",
            data = fake_data,
            add = c("mean_ci", "jitter"),
            palette = "jco",
            color = "treatment")

```

What the distribution of the fake data looks like

```{r, message=FALSE, warning=FALSE}
n <- 10^4
qplot(rnegbin(n, mu=10, theta=1))
```

A linear model to estimate the treatment effect.

```{r}
knitr::kable(coef(summary(lm(count ~ treatment, data=fake_data))),
             digits = c(1, 1, 1, 3))
```

A non-parametric (Mann-Whitney-Wilcoxon) test

```{r}
wilcox.test(count ~ treatment, data=fake_data)
```


Finally, a permutation, test. The basic algorithm is

1. compute the test statistic for the observed data, assign this to $\theta_1$
2. permute the response
3. compute the test statistic for the permuted data, assign these to $\theta_{2..m}$
4. repeat 2 and 3 $m-1$ times
5. compute $p$ as

\begin{equation}
p_{perm} = \frac{N_{\theta_i \ge \theta_{1}}}{m}
\end{equation}

This is easily done with a **for loop** in which the observed statistic is the first value in the vector of statistics. If this is done, the minimum value in the numerator for the computation of $p_{perm}$ is 1, which insures that $p_{perm}$ is not zero.

The test statistic depends on the analysis. For the simple comparison of means, a simple test statistic is the difference in means. This is the numerator of the test statistic in a *t*-test. The test has more power if the test-statistic is scaled (Manley xxx), so a better test statistic would be *t*, which scales the difference by its standard error.

Here, I implement this algorithm. The test is two-tailed, so the absolute difference is recorded. The first value computed is the observed absolute difference.

```{r}
set.seed(1)
n_permutations <- 5000
d <- numeric(n_permutations)

# create a new column which will contain the permuted response
# for the first iteration, this will be the observed order
fake_data[, count_perm := count]

for(i in 1:n_permutations){
  d[i] <- abs(t.test(count_perm ~ treatment, data = fake_data)$statistic)
  
  # permute the count_perm column for the next iteration
  fake_data[, count_perm := sample(count)]
}
p <- sum(d >= d[1])/n_permutations
p
```

#### Some R packages with permutation tests.

`lmPerm::lmp` generates permutation p-values for parameters of any kind of linear model. The test statistic is the sum of squares of the term scaled by the residual sum of squares of the model.

```{r}
coef(summary(lmp(count ~ treatment, perm="Prob", Ca=0.01, 
                 data=fake_data)))
```

### Performance of parametric tests and alternatives

#### Type I error

If we are going to compute a $p$-value, we want it to be uniformly distributed "under the null". A simple way to check this is to compute Type I error. If we set $\alpha = 0.05$, then we'd expect 5% of tests of an experiment with no effect to have $p < 0.05$.

```{r, comment=FALSE, results='hide'}
# first create a matrix with a bunch of data sets, each in its own column
n <- 10
n_sets <- 4000
fake_matrix <- rbind(matrix(rnegbin(n*n_sets, mu=10, theta=1), nrow=n),
                   matrix(rnegbin(n*n_sets, mu=10, theta=1), nrow=n))
treatment <- rep(c("cn", "tr"), each=n)

tests <- c("lm", "log_lm","mww", "perm")
res_matrix <- matrix(NA, nrow=n_sets, ncol=length(tests))
colnames(res_matrix) <- tests
for(j in 1:n_sets){
  res_matrix[j, "lm"] <- coef(summary(lm(fake_matrix[,j] ~ treatment
                                 )))[2, "Pr(>|t|)"]
  res_matrix[j, "log_lm"] <- coef(summary(lm(log(fake_matrix[,j] + 1) ~ treatment
                                 )))[2, "Pr(>|t|)"]
  res_matrix[j, "mww"] <- wilcox.test(fake_matrix[,j] ~ treatment,
                                      exact=FALSE)$p.value
  res_matrix[j, "perm"] <- coef(summary(lmp(fake_matrix[,j] ~ treatment,
                                 perm="Prob", Ca=0.01)))[2, "Pr(Prob)"]
}

```

```{r}
apply(res_matrix, 2, function(x) sum(x < 0.05)/n_sets)
```

Type I error is computed for the linear model, the linear model with a log transformed responpse, Mann-Whitney-Wilcoxon, and permutation tests. All four tests are slightly conservative for data that look like that modeled. The computed Type I error of the permutation test is closest to the nominal value of 0.05.

#### Power

If all we care about is a $p-value$ then we want to use a test that is most powerful.

```{r, comment=FALSE, results='hide'}
# first create a matrix with a bunch of data sets, each in its own column
n <- 5
n_sets <- 4000
fake_matrix <- rbind(matrix(rnegbin(n*n_sets, mu=10, theta=1), nrow=n),
                   matrix(rnegbin(n*n_sets, mu=20, theta=1), nrow=n))
treatment <- rep(c("cn", "tr"), each=n)

tests <- c("lm", "log_lm","mww", "perm")
res_matrix <- matrix(NA, nrow=n_sets, ncol=length(tests))
colnames(res_matrix) <- tests
for(j in 1:n_sets){
  res_matrix[j, "lm"] <- coef(summary(lm(fake_matrix[,j] ~ treatment
                                 )))[2, "Pr(>|t|)"]
  res_matrix[j, "log_lm"] <- coef(summary(lm(log(fake_matrix[,j] + 1) ~ treatment
                                 )))[2, "Pr(>|t|)"]
  res_matrix[j, "mww"] <- wilcox.test(fake_matrix[,j] ~ treatment,
                                      exact=FALSE)$p.value
  res_matrix[j, "perm"] <- coef(summary(lmp(fake_matrix[,j] ~ treatment,
                                 perm="Prob", Ca=0.01)))[2, "Pr(Prob)"]
}

```

```{r}
apply(res_matrix, 2, function(x) sum(x < 0.05)/n_sets)
```

As above, Power is computed for the linear model, linear model with a log-transformed response, Mann-Whitney-Wilcoxan, and permutation, by simulating a "low power" experiment. The effect is huge (twice as many cells) but the power is low because the sample size is small ($n = 5$). At this sample size, and for this model of fake data, all tests have low power. The power of the log-transformed response is the largest. A problem is, this is not a test of the means but of the log transformed mean plus 1. The power of the permutation test is about 25% larger than that of the linear model and Mann-Whitney-Wilcoxan test. An advantage of this test is that it is a p-value of the mean. A good complement to this p-value would be bootstraped confidence intervals. Repeat this simulation using $n=40$ do see how the relative power among the three change in a simulation of an experiment with more power.

## max vs. mean

## pre-post, normalization
