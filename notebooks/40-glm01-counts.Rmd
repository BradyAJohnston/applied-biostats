---
title: "Generalized linear models I: Count data"
author: "Jeffrey A. Walker"
date: "11/12/2018"
output: html_document
---
# Generalized linear models I: Count data
```{r glm1-setup, echo=FALSE, message=FALSE}
library(ggplot2)
library(ggpubr)
library(cowplot)
library(readxl)
library(emmeans)
library(data.table)

library(MASS) # negative binomial
library(vcd)
library(DHARMa)


data_path <- "../data" # notebook
data_path <- "data" # bookdown

#source("../R/clean_label.R")
source("R/clean_label.R") # bookdown

```

```{r glm1-testdata1, echo=FALSE, eval=FALSE}
# not finding good model

folder <- "Data from Environmentally dependent host-pathogen and vector-pathogen interactions"
file <- "Davis et al JAE data.xlsx"
which_sheet <- "Exp 3 Acute stress"
file_path <- paste(data_path, folder, file, sep="/")
wheat.sheet3 <- data.table(read_excel(file_path, sheet=which_sheet))
colnames(wheat.sheet3) <- clean_label(colnames(wheat.sheet3))

y <- "n_seeds_germinating"
fit <- lm(get(y) ~ Treatment, data=wheat.sheet3)
plot(fit)
qplot(fit$residuals)
emmeans(fit, specs="Treatment")
contrast(emmeans(fit, specs="Treatment"), method="revpairwise")
simulationOutput <- simulateResiduals(fittedModel = fit, n = 250)
plot(simulationOutput)
testUniformity(simulationOutput = simulationOutput)
testDispersion(simulationOutput)

fit.good <- goodfit(wheat.sheet3$n_seeds) 
summary(fit.good) 
rootogram(fit.good)
Ord_plot(wheat.sheet3$n_seeds)
distplot(wheat.sheet3$n_seeds, type="poisson")

fit.glm <- glm(get(y) ~ Treatment, family=poisson, data=wheat.sheet3)
emmeans(fit.glm, specs="Treatment")
contrast(emmeans(fit.glm, specs="Treatment"), method="revpairwise")
contrast(emmeans(fit.glm, specs="Treatment"), method="revpairwise", type="response")
simulationOutput <- simulateResiduals(fittedModel = fit.glm, n = 250)
plot(simulationOutput)
testUniformity(simulationOutput = simulationOutput)
testDispersion(simulationOutput)

fit.nb <- glm.nb(get(y) ~ Treatment, data=wheat.sheet3)
emmeans(fit.nb, specs="Treatment")
contrast(emmeans(fit.nb, specs="Treatment"), method="revpairwise")
contrast(emmeans(fit.nb, specs="Treatment"), method="revpairwise", type="response")
simulationOutput <- simulateResiduals(fittedModel = fit.nb, n = 250)
plot(simulationOutput)
testUniformity(simulationOutput = simulationOutput)
testDispersion(simulationOutput)

```

```{r glm1-testdata2, echo=FALSE, eval=FALSE}
# nice results

folder <- "Data from Sugar provisioning maximizes the biocontrol service of parasitoids"
file <- "4_Parastism_Fig_6.csv"
file_path <- paste(data_path, folder, file, sep="/")
sugar <- fread(file_path)
sugar[, Treatment:=factor(Treatment, c("Control", "Releases", "Releases_sugar"))]
qplot(x=Treatment, y=Parasitized, data=sugar) + geom_violin()
fit <- glm.nb(Parasitized ~ Treatment, data=sugar)
(emm <- emmeans(fit, specs="Treatment"))
contrast(emm, method="revpairwise")
anova(fit)

plot(fit)
simulationOutput <- simulateResiduals(fittedModel = fit, n = 250)
plot(simulationOutput, asFactor = F)
testUniformity(simulationOutput = simulationOutput)
testDispersion(simulationOutput)

```

```{r glm1-testdata3, echo=FALSE, eval=FALSE}
# Excellent!

folder <- "Data from Experimental parasite community ecology- intraspecific variation in a large tapeworm affects community assembly"
file <- "Lab_exp.csv"
file_path <- paste(data_path, folder, file, sep="/")
worm <- fread(file_path)
worm[, Treatment:=factor(Treatment, c("Control", "Uninfected", "Infected LG", "Infected HG"))]
ggerrorplot(x="Treatment", y="Diplo_intensity", data=worm, add=c("mean", "jitter"), desc_stat = "mean_se", error.plot = "errorbar")

fit <- glm.nb(Diplo_intensity ~ Treatment, data=worm)
(emm <- emmeans(fit, specs="Treatment", type="response"))
contrast(emm, method="revpairwise")
anova(fit)

plot(fit)
simulationOutput <- simulateResiduals(fittedModel = fit, n = 250)
plot(simulationOutput, asFactor = F)
testUniformity(simulationOutput = simulationOutput)
testDispersion(simulationOutput)

# regression to the mean
worm[, change_weight:=Final_weight_g-Initial_weight_g]
coef(summary(lm(Final_weight_g ~ Treatment, data=worm)))
coef(summary(lm(change_weight ~ Treatment, data=worm)))
coef(summary(lm(change_weight ~ Initial_weight_g + Treatment, data=worm)))

```

Biologists frequently count stuff, and design experiments to estimate the effects of different factors on these counts. For example, the effects of environmental mercury on clutch size in a bird, the effects of warming on parasite load in a fish, or the effect of exercise on RNA expression.

Count data differ from data with Normal error in many ways, including 1) counts are discrete 2) counts tend to bunch up on the small side of the range, creating a distribution with a positive skew, 3) counts can be zero, and a sample of counts can have an abundance of zeros, and 4) the variance of count data tends to increase with the mean (see Figure \@ref(fig:glm1-plot1) for some of these properties). Some count data can be approximated by a normal distribution and reasonably modeled with a linear model but more often, count data are modeled with something other than a normal distribution using a **generalized linear model** (GLM). Back before modern computing and fast processors, count data were often analyzed by either **transforming** the response or by **non-parametric hypothesis tests**. I prefer a GLM because both the analysis of transformed data and non-parametric hypothesis tests are really a response to "correct" $p$-values instead of interpretable parameter estimates.

```{r glm1-plot1, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Histogram of the count of a trematode parasite larvae in Control vs. Infected fish. Fish in the Infected treatment are infected with a tapeworm."}

folder <- "Data from Experimental parasite community ecology- intraspecific variation in a large tapeworm affects community assembly"
file <- "Lab_exp.csv"
file_path <- paste(data_path, folder, file, sep="/")
worm <- fread(file_path)
worm[, Treatment:=factor(Treatment, c("Control", "Uninfected", "Infected LG", "Infected HG"))]

gghistogram(data=worm[Treatment %in% c("Control", "Infected HG")], x = "Diplo_intensity",
   color="Treatment", fill="Treatment",
   add = "mean", rug = FALSE,
   palette = c("#00AFBB", "#E7B800"))

```

## The generalized linear model

Section [Assumptions for inference with statistical models] in Chapter 1 introduced two ways of defining a statistical model fit to data.

1. Definition using a distribution of the error term

\begin{align}
Y &= \beta_0 + \beta_1 X + \varepsilon\\
\varepsilon &\sim N(0, \sigma)
(\#eq:lm-again)
\end{align}

2. Definition using the response as a combination of stochastic and deterministic components

\begin{align}
Y &\sim N(\mu, \sigma)\\
\mu &= \beta_0 + \beta_1 X
(\#eq:lm-spec2)
\end{align}

A generalized linear model has has these two parts of the second definition but adds a third part

1. A probability distribution from the exponential family (this is the stochastic part)
\begin{equation}
Y \sim P(\mu)
\end{equation}
2. a linear predictor of the form (this is the deterministic part)
\begin{equation}
\eta=\beta_0 + \beta_1 X
\end{equation}
3. a **link function** connecting the two parts
\begin{equation}
\eta = g(\mu)
\end{equation}

$\mu$ (the Greek symbol mu) is the conditional mean (or expectation $\mathrm{E}(Y|X)$) of the response on the **response scale** and $\eta$ is the conditional mean of the response on the **link scale**. A GLM models the response with a distribution specified by the probability distribution using the link function. The probability distributions introduced here are the Poisson and Negative Binomial for count data, and the Binomial for binary data. Note that a linear model is a GLM with a link to a Normal distribution.

The link scale is linear (it is the log of the response scale), and so the effects are additive on the link scale, while the response scale is nonlinear (it is the exponent of the link scale), and so the effects are multiplicative on the response scale. If this doesn't make sense now, an example is worked out below. The inverse of the link function backtransforms the parameters from the link scale back to the response scale. So, for example, a prediction on the response sale is $\mathrm{exp}(\hat{\eta})$ and a coefficient on the response scale is $\mathrm{exp}(b_j)$.

## Count data example 

The example is an experiment measuring the effect of the parasitic tapeworm *Schistocephalus solidus* infection on the susceptibility of infection from a second parasite, the trematode *Diplostomum pseudospathaceum*, in the threespine stickleback fish *Gasterosteus aculeatus*. The treatment levels are "Control" (unexposed to the tapeworm), "Uninfected" " (exposed to the tapeworm but uninfected), "Infected LG" (exposed and infected with the low growth population of the tapeworm), and "Infected HG" (exposed and infected with the high growth population of tapeworm). The response is the number of trematode larvae counted in the eyes (right and left combined) of the fish. A histogram of the counts is shown in Figure \@ref(fig:glm1-plot1) for the control and Infected HG treatment levels.

I start the analysis by fitting a linear model to the worm data and then *model checking*, to gain some insight on how well the linear model fits the data

 <div style="background-color:#cccccc; text-align:left; vertical-align: middle; padding:20px 47px;">
**NHST blues** Students are often encouraged by textbooks, colleagues, or the literature to start the analysis by first "testing" assumptions with hypothesis tests -- for example using a Shaprio-Wilks test of normality as a decision rule to decide if to use a parametric test such as a $t$-test or ANOVA if the null hypothesis of normality is not rejected, or a non-parametric test such as a Mann-Whitney U test if the null hypothesis of normality is rejected. I advise against this, because 1) this pre-test filtering automatically invalidates the $p$-value of the hypothesis test as it does not adjust for the filtering procedure, 2) real data are only approximately normal and as $n$ increses, a normality test will reject any real dataset, and 3) hypothesis tests are pretty robust to non-normality anyway.
</div> 

The model is

\begin{align}
Y &= N(\mu, \sigma)\\
\mu &= \beta_0 + \beta_1 Treatment
\end{align}

Which models the parasite count conditional on $Treatment$ with a Normal distribution. Remember, this is equivalent to a model with an error term with a Normal distribution (now might be a good time to go back to Chapter 1 and review the section [Assumptions for inference with statistical models]).

### Checking the model I -- a Normal Q-Q plot

```{r glm1-plot2, echo=FALSE, message=FALSE, warning=FALSE}
fit.lm <- lm(Diplo_intensity ~ Treatment, data=worm, na="na.exclude")
worm[, Diplo_intensity_residual:=residuals(fit.lm)]
gg1 <- gghistogram(data=worm, x = "Diplo_intensity_residual",
   add = "mean", rug = FALSE,
   color = c("#00AFBB"), fill = c("#00AFBB"))
gg2 <- ggqqplot(data=worm, x = "Diplo_intensity_residual", title="Normal Q-Q")
plot_grid(gg1, gg2, labels="AUTO")
```

Figure \@ref(fig:glm1-plot2)A shows a histogram of the residuals from the fit linear model. The plot shows that the residuals are clumped at the negative end of the range, which suggests that a model with a normally distributed conditional outcome (or normal error) is not well approximated.

A better way to investigate this is with the **Normal Q-Q** plot in Figure \@ref(fig:glm1-plot2)B, which plots the sample quantiles for a variable against their theoretical quantiles. If the conditional outcome approximates a normal distribution, the points should roughly follow the line. Instead, for the worm data, the points are above the line at both ends. At the left (negative) end, this means that we aren't seeing the most negative values that would be expected (the observed values are more positive than the theoretical values). Remembering that this plot is of residuals, if we think about this as counts, this means that our smallest counts are not as small as we would expect given the mean and a normal distribution. This shouldn't be surprising -- the counts range down to zero and counts cannot be below zero. At the positive end, the sample values are again more positive than the theoretical values. Thinking about this as counts, this means that are largest counts are larger than expected given the mean and a normal distribution. This pattern is exactly what we'd expect of count data, or at least count data that borders zero.

 <div style="background-color:#cccccc; text-align:left; vertical-align: middle; padding:20px 47px;">
**Intuition Pump** Let's construct a Normal Q-Q plot. A **quantile** (or percentile) of a vector of numbers is the value of the point at a specified percentage rank. The median is the 50% quantile. The 95% confidence intervals are at the 2.5% and 97.5% quantiles. In a Normal Q-Q plot, we want to plot the quantiles of the residuals against a set of theoretical quantiles.

1. To get the observed quantiles, rank the residuals of the fit linear model from most negative to most positive -- these are your quantiles! For example, if you have $n=145$ residuals, then the 73rd point is the 50% quantile.
2. A theoretical quantile from the Normal distribution can be constructed using the `qnorm` function which returns the Normal quantiles for a specified vector of percents. Alternatively, one could randomly sample $n$ points using `rnorm`. These of course will be sampled quantiles so will only approximate the expected theoretical quantiles, but I add this here because we use this method below.

Now simply plot the observed against theoretical quantiles. Often, the **standardized** quantiles are plotted. A standardized variable has a mean of zero and a standard deviation of one and is computed by 1) centering the vector at zero by subtracting the mean from every value, and 2) dividing each value by the standard deviation of the vector. Recognize that because a standard deviation is a function of deviations from the mean, it doesn't matter which of these operations is done first. A standardized theoretical quantile is specified by `qnorm(p, mean = 0, sd = 1)`, which is the default.

Below, I've plotted the standardized observed and theoretical quantiles against the vector of percents (from 0 to 100%). This plot also nicely shows how the residuals of the worm data deviate from that expected if these had a normal distribution. The plot nicely shows that the most negative observed quintiles are not as negative as expected given a normal distribution, which again makes sense because this would imply negative counts since the mean is close to zero. And it nicely shows that the most positive observed quantiles are more positive than expected given a normal distribution, again this makes sense in right skewed count data. Finally, the plot nicely shows that the median is less positive than that expected given a normal distribution, which is at the mean (a right skew tends to pull the mean to the right of the median).

```{r glm1-qqplot-sim, echo=FALSE}
Observed <- scale(sort(worm[, Diplo_intensity_residual]))
n <- length(Observed)
q <- n+1
x <- seq(1/q, 1-1/q,by=1/q)
Theoretical <- qnorm(x)
qq_data <- data.table(X=rep(x,2),
                      Data=rep(c("Observed", "Theoretical"), each=n), 
                      Quantile=c(Observed, Theoretical))
gg <- ggplot(data=qq_data, aes(x=X, y=Quantile, color=Data)) +
  geom_point() +
  NULL
gg
#qplot(x=Theoretical, y=Observed) + geom_abline(slope=1, intercept=0)
```
</div> 

### Checking the model II -- scale-location plot for checking homoskedasticity

```{r glm1-worm-scale-location}
worm[, scale_y:= sqrt(abs(scale(Diplo_intensity_residual)))]
worm[, fitted_y:=fitted(fit.lm)]
ggscatter(data=worm, 
          x="fitted_y",
          y="scale_y",
          title="Scale-Location",
          xlab="Fitted value",
          ylab="sqrt(standardized residual)",
          add="reg.line",
          conf.int = TRUE,
          add.params = list(color = "blue",
                            fill = "lightgray")
          )
```

A linear model also assumes the error has constant variance (that is, the error variance is not a function of the value of $X$), or homoskedasticity. The fit model can be checked for homoskedasticity using a scale-location plot, which is a scatterplot of the positive square-root of the standardized residuals against the fitted values^[fitted values are the predicted values, $\hat{Y}$]. If the residuals approximate a normal distribution, then a regression line through the scatter should be close to horizontal. The regression line in the scale-location plot of the fit of the linear model to the worm data shows a distinct increase in the "scale" (the square root of the standardized residuals) with increased fitted value, which is expected of data that are lognormally, Poisson, or negative binomially distributed.

### Two distributions for count data -- Poisson and Negative Binomial

The pattern in the Normal Q-Q plot in Figure \@ref(fig:glm1-plot2)B should discourage one from modeling the data with a normal distribution and instead model the data with an alternative distribution using a Generalized Linear Model. There is no unique mapping between how data are generated and a specific distribution, so this decision is not as easy as thinking about the data generation mechanism and then simply choosing the "correct" distribution. Section 4.5 in Bolker (xxx) is an excellent summary of how to think about the generating processes for different distributions in the context of ecological data. Since the response in the worm data are counts, we need to choose a distribution that generates integer values, such as the Poisson or the negative binomial.

1. Poisson -- A Poisson distribution is the probability distribution of the number of occurrences of some thing (an egg, a parasite, or a specific mRNA transcript) generated by a process that generates the thing at a constant rate per unit effort (duration or space). This constant rate is $\lambda$, which is the expectation, so $\mathrm{E}(Y) = \mu = \lambda$. Because the rate per effort is constant, *the variance of a Poisson variable equals the mean*, $\sigma^2 = \mu = \lambda$. Figure \@ref(fig:glm1-poisson) shows three samples from a Poisson distribution with $\lambda$ set to 1, 5, and 10. The plots show that, as the mean count ($\lambda$) moves away from zero, a Poisson distribution 1) becomes less skewed and more closely approximates a normal distribution and 2) has an increasingly low probability of including zero (less than 1% zeros when the mean is 5).

A Poisson link function, then, is useful for count data in which the conditional variance is close to the conditional mean. Very often, biological count data are not well approximated by a Poisson distribution because the variance is either less than the mean, an example of **underdispersion**^[the variance is less than that expected by the probability model], or greater than the mean, an example of **overdispersion**^[the variance is greater than that expected by the probability model]. A useful distribution for count data with overdispersion is the negative binomial.


```{r glm1-poisson-ci, echo=FALSE, eval=FALSE}
# interesting results - essentially normal does better than poisson
n <- 20
exp_b0 <- 1000
exp_b1 <- 1000
x <- rep(c("a", "b"), each=n)
niter <- 1000
b1 <- matrix(NA, nrow=niter, ncol=2)
b1.glm <- matrix(NA, nrow=niter, ncol=2)
for(iter in 1:niter){
  y <- c(rpois(n, lambda=exp_b0), rpois(n, lambda=(exp_b0+exp_b1)))
  fit <- lm(y~x)
  b1[iter,] <- confint(fit)[2,]
  fit.glm <- glm(y~x, family="poisson")
  b1.glm[iter,] <- exp(confint(fit.glm)[2,] + coef(fit.glm)[1])  - exp(coef(fit.glm)[1])
  #b1.glm2[iter,] <- exp(confint(fit.glm))[2,]*exp(coef(fit.glm)[1]) - exp(coef(fit.glm)[1])
}
sum((exp_b1 > b1[,1] & exp_b1 < b1[, 2]))/niter*100
sum((exp_b1 > b1.glm[,1] & exp_b1 < b1.glm[, 2]))/niter*100
```


```{r glm1-poisson, echo=FALSE}
n_samp <- 10^4
rp1 <- data.frame(y=rpois(n=n_samp, lambda=1))
rp5 <- data.frame(y=rpois(n=n_samp, lambda=5))
rp10 <- data.frame(y=rpois(n=n_samp, lambda=10))
fake_data_wide <- data.table(lambda.eq.1=rp1[,1], lambda.eq.5=rp5[,1], lambda.eq.10=rp10[,1])
fake_data_wide <- data.table("lambda = 1"=rp1[,1], "lambda = 5" =rp5[,1], "lambda = 10" =rp10[,1])
fake_data <- data.table(lambda=rep(c("1", "5", "10"), each=n_samp), count=c(lambda.eq.1=rp1[,1], lambda.eq.5=rp5[,1], lambda.eq.10=rp10[,1]))

zeros <- fake_data[, .(zeros=length(which(count==0))/n_samp*100), by=.(lambda)]
# gg <- gghistogram(data=fake_data,
#                   x = "count",
#                   add = "mean",
#                   binwidth=1,
#                   color="lambda",
#                   fill = "lambda",
#                   palette = rep("#00AFBB",3),
#                 )
# facet(gg, facet.by = "lambda")

gghistogram(data=fake_data_wide,
            x = c("lambda = 1", "lambda = 5", "lambda = 10"),
            add = "mean",
            combine=TRUE,
            binwidth=1,
            color = c("#00AFBB"), fill = c("#00AFBB"),
            xlab = "count",
            ylab =FALSE
                )

```

2. Negative Binomial -- The negative binomial distribution is a discrete probability distribution of the number of successes that occur before a specified number of failures $k$ given a probability $p$ of success. This isn't a very useful way of thinking about modeling count data in biology. What is useful is that the Negative Binomial distribution can be used simply as way of modeling an "overdispersed" Poisson process. The mean of a negative binomial variable is $\mu = k\frac{p}{1-p}$ and the variance is  $\sigma^2 = \mu + \mu^2/k$. As a method for modeling an overdispersed Poisson variable, $k$ functions as a parameter controlling the amount of overdispersion and can be any real, positive value (not simply a positive integer), including values less than 1.

```{r glm1-test2, echo=FALSE, eval=FALSE}
n_samp <- 10^4
rp1 <- data.frame(y=rnbinom(n=n_samp, mu=10, size=0.9))
rp5 <- data.frame(y=rnbinom(n=n_samp, mu=10, size=10))
rp10 <- data.frame(y=rnbinom(n=n_samp, mu=100, size=0.9))
fake_data_wide <- data.table(lambda.eq.1=rp1[,1], lambda.eq.5=rp5[,1], lambda.eq.10=rp10[,1])
fake_data_wide <- data.table("mu=10, k=0.9"=rp1[,1], "mu=10, k=10" =rp5[,1], "mu=100, k=0.9" =rp10[,1])

gghistogram(data=fake_data_wide,
            x = c("mu=10, k=0.9", "mu=10, k=10", "mu=100, k=0.9"),
            add = "mean",
            combine=TRUE,
            binwidth=1,
            color = c("#00AFBB"), fill = c("#00AFBB"),
            xlab = "count",
            ylab =FALSE,
            scales="free"
                )

```

### Fitting a GLM with a Poisson link to the worm data

Let's fit a GLM with a Poisson lin to the worm data. The model is

\begin{align}
Diplo\_intensity &\sim Poisson(\mu)\\
\mathrm{E}({Diplo\_intensity|Treatment}) &= \mu\\
\mu &= \mathrm{exp}(\eta)\\
\eta &= \beta_0 + \beta_1 Uninfected + \beta_2 Infected\_LG + \beta_3 Infected\_HG
\end{align}

1. The first line of the model is the stochastic part stating the response is modeled as a random Poisson variable with mean and variance $\mu$ (the rate parameter $\lambda$ of the Poisson distribution).
2. The second line states the $\mu$ is the conditional mean or conditional expectation
3. The third line connects the conditional mean on the link scale ($\eta$) with the conditional mean on the response scale ($\mu$)
4. The fourth line is the linear predictor, and includes three dummy variables.

For model checking a GLM model fit, an alternative to a Normal Q-Q plot is a Quantile residual Q-Q plot of observed vs. expected **quantile residuals**. The basic algorithm for this is

1. Use the model parameters to simulate $p$ fake values of the response for each row of the data. This will be a $n \times p$ matrix of fake data where each column is a new, random sample of a population with parameters equal to that estimated by the observed data.
2. For each observation (each row of the matrix of fake data), compute the fraction of simulated values smaller than the observed value of the response variable for that row. This fraction is the observed **quantile residual**, which ranges in value from 0 to 1. If the true data are distribitued as that specified by the model, then quantile residuals will have a uniform distribution -- any value within the range 0 to 1 is equally likely.
3. Sort the observed quantile residuals from smallest to largest and plot against theoretical quantile residuals from a uniform distribution. One could transform the quantile residuals to standard, normal residuals and then plot using a traditional Normal Q-Q plot but this step isn't necessary (if reported, a Normal Q-Q plot of transformed quantile residuals might confuse readers who failed to read the fine print).

 <div style="background-color:#cccccc; text-align:left; vertical-align: middle; padding:20px 47px;">
 A common misconception is that if the distribution of the response approximates a Poisson distribution, then the residuals of a GLM fit with a Poisson link should be normally distributed, which could then be checked with a Normal Q-Q plot, and homoskedastic, which could be checked with a scale-location plot. Neither of these is true because a GLM does not transform the data and, in fact, the model definition does not specify anything about the distribution of an "error" term -- there is no $\varepsilon$ in the model defintion above! This is why thinking about the definition of a linear model by specifying an error term with a normal distribution can be confusing and lead to misconceptions when learning GLMs.
</div> 

```{r glm1-worm-poisson-qr, echo=FALSE, message=FALSE}
n_sim <- 250
fit.pois <- glm(Diplo_intensity ~ Treatment, family="poisson", data=worm)
plot_it <- FALSE
if(plot_it==TRUE){
  simulationOutput <- simulateResiduals(fittedModel = fit.pois, n = n_sim)
  plot(simulationOutput, asFactor = F)
}

# do this by hand
plot_it <- TRUE
n_row <- nrow(worm)

if(plot_it==TRUE){
  fake_data <- cbind(worm[, Diplo_intensity], simulate(fit.pois, nsim = n_sim, seed = 101))
  
  # do fake data by hand
  fake_data_2 <- matrix(NA, nrow=n_row, ncol=n_sim)
  for(j in 1:n_sim){
    fake_data_2[,j] <- rpois(n_row, lambda=fitted(fit.pois))
  }
  fake_data_2 <- cbind(worm[, Diplo_intensity], fake_data_2)
  # this works

  quantile_residuals <- numeric(n_row)
  for(i in 1:n_row){
    quantile_residuals[i] <- (rank(fake_data[i,])[1] - 1)/n_sim
  }
  Observed <- sort(quantile_residuals)
  q <- n_row+1
  x <- seq(1/q, 1-1/q,by=1/q)
  Theoretical <- qunif(x)
  ggscatter(data=data.frame(Theoretical=Theoretical, Observed=Observed),
         x="Theoretical", 
         y="Observed",
         title = "Quantile Residual Q-Q Plot"
  ) +
    geom_abline(slope=1, intercept=0) +
    NULL
  
}

# convert to normal with box-mueller
# u1 <- quantile_residuals
# u2 <- runif(n_row)
# z <- sqrt(-2*log(u1))*cos(2*pi*u2)
# worm[, fitted_pois:=fitted(fit.pois)]
# worm[, scale_pois:=sqrt(abs(z))]
# ggscatter(data=worm, 
#           x="fitted_pois",
#           y="scale_pois",
#           title="Scale-Location",
#           xlab="Fitted value",
#           ylab="sqrt(standardized residual)",
#           add="reg.line",
#           conf.int = TRUE,
#           add.params = list(color = "blue",
#                             fill = "lightgray")
#           )

```

The Q-Q plot using quantile residuals with a Poisson link indicates that the counts of *Diplostomum* larvae in the eyes of the threespine stickleback are not well approximated by a Poisson distribution.

### Fitting a GLM with a Negative Binomial link to the worm data

The model is

\begin{align}
Diplo\_intensity &\sim NB(\mu, k)\\
\mathrm{E}({Diplo\_intensity|Treatment}) &= \mu\\
\mu &= \mathrm{exp}(\eta)\\
\eta &= \beta_0 + \beta_1 Uninfected + \beta_2 Infected\_LG + \beta_3 Infected\_HG
\end{align}

This model specifies a negative binomial link but otherwise is just like that above specifying a Poisson link.

#### Model checking

```{r glm1-worm-nb, echo=FALSE}
n_sim <- 250
fit.nb <- glm.nb(Diplo_intensity ~ Treatment, data=worm)

plot_it <- FALSE
if(plot_it==TRUE){
  simulationOutput <- simulateResiduals(fittedModel = fit.nb, n = n_sim)
  plot(simulationOutput, asFactor = F)
  # testUniformity(simulationOutput = simulationOutput)
  # testDispersion(simulationOutput)
}

# do this by hand
plot_it <- TRUE
if(plot_it==TRUE){
  # do fake data by hand
  fake_data <- matrix(NA, nrow=n_row, ncol=n_sim)
  for(j in 1:n_sim){
    fake_data[,j] <- rpois(n_row, lambda=fitted(fit.pois))
    fake_data[,j] <- rnegbin(n_row, mu=fitted.values(fit.nb), theta=fit.nb$theta)
  }
  fake_data <- cbind(worm[, Diplo_intensity], fake_data)
  # this works

  n_row <- nrow(worm)
  quantile_residuals <- numeric(n_row)
  for(i in 1:n_row){
    quantile_residuals[i] <- (rank(fake_data[i,])[1] - 1)/n_sim
  }
  Observed <- sort(quantile_residuals)
  q <- n_row+1
  x <- seq(1/q, 1-1/q,by=1/q)
  Theoretical <- qunif(x)
  ggscatter(data=data.frame(Theoretical=Theoretical, Observed=Observed),
         x="Theoretical", 
         y="Observed",
         title = "Quantile Residual Q-Q Plot"
  ) +
    geom_abline(slope=1, intercept=0) +
    NULL
  
}

```

A quantile residual Q-Q plot of the GLM model fit with negative binomial link is illustrated above. This looks pretty good.

#### Model means and coefficients

In a Generalized Linear Model of counts using either a Poisson or negative binomial link, modeled means, coefficients, and contrasts can be reported either on the link or response scale. Remember, the response scale is a count, while the link scale is a log(count).

The modeled means on the link scale are

```{r glm1-worm-means-link, echo=FALSE}
# cell means link space
emmeans(fit.nb, specs="Treatment")
```

While the means on response scale are
```{r glm1-worm-means-response, echo=FALSE}
emmeans(fit.nb, specs="Treatment", type="response")
```

1. A mean on the response scale is simply the exponent of the mean on the link scale. For example, the mean of the Control treatment level on the response scale is $\mathrm{exp}(1.821408) = 6.180555$.

2. The CIs on the link scale are symmetric around the mean but those on the response scale are not. This is a feature, not a bug. Remember that counts are right skewed which means a CI will have a wider right than left interval. Check this!

3. If a plot includes a 1 SE error bar on the response scale, this is technically correct but it encourages the practice of computing CIs using the 2*SE rule of thumb. This rule breaks down for count data with right skewed distributions.

4. Plotting the response scale CIs is both technically correct and makes the 2*SE rule of thumb unnecessary.

```{r glm1-worm-nb-ci, echo=FALSE, eval=FALSE}
# link scale
link <- data.table(summary(emmeans(fit.nb, specs="Treatment")))
link[Treatment=="Control",asymp.UCL] - link[Treatment=="Control", emmean]
link[Treatment=="Control",asymp.LCL] - link[Treatment=="Control", emmean]
response <- data.table(summary(emmeans(fit.nb, specs="Treatment", type="response")))
response[Treatment=="Control",asymp.UCL] - response[Treatment=="Control", response]
response[Treatment=="Control",asymp.LCL] - response[Treatment=="Control", response]

```

The model coefficients on the link scale are

```{r glm1-worm-nb-coeffs-link, echo=FALSE, message=FALSE}
#cbind(coefficients(summary(fit.nb)), confint(fit.nb))
emm <- emmeans(fit.nb, specs="Treatment")
summary(contrast(emm, method="trt.vs.ctrl", adjust="none"), infer=c(TRUE, TRUE))
```

and on the response scale

```{r glm1-worm-nb-coeffs-response, echo=FALSE}
emm <- emmeans(fit.nb, specs="Treatment")
summary(contrast(emm, method="trt.vs.ctrl", adjust="none", type="response"), infer=c(TRUE, TRUE))
```

Notice that the emmeans package reports the name of the term as the ratio of the coefficient term to the intercept term (the reference treatment level). This is because the coefficients on the response scale *are* ratios and effects are not additive but multiplicative! So, for example, the mean of the Infected HG treatment level on the response scale is $b_0*b_3$ (remember that with a linear model the mean would be $b_0 + b_3$). Check and see if this works.

The effects (the coefficients) could be reported as these ratios in a table, or in the text it could be reported something like "Infected HG fish had 71.5% (95%CI: 38.9% - 111.8%) more *Diplostomum* larvae than Control fish." Where do these percents come from? The percent effect is $100(b_j - 1)$ larger than the reference mean if the $b_j > 1$ or $100(1 - b_j)$ smaller than the reference mean if the $b_j < 1$.


```{r glm1-worm-percent, echo=FALSE, eval=FALSE}

emm.response <- data.table(summary(emmeans(fit.nb, specs="Treatment", type="response")))
coef.response <- exp(coef(fit.nb))

# notice that exp(b0) = control mean
# so Uninfected is 27.2% smaller than the control
# this number is 100(1-b1)
100*(1-coef.response[2])
# does it work? This should equal the Uninfected mean
emm.response[Treatment=="Control", response]*coef.response[2]
```

## Working in R

Source publication: Benesh, D. P., & Kalbe, M. (2016). Experimental parasite community ecology: intraspecific variation in a large tapeworm affects community assembly. Journal of Animal Ecology, 85(4), 1004-1013.

Source data URL: https://datadryad.org/resource/doi:10.5061/dryad.bq8j8

Source file: "Lab_exp.csv"

Poisson fit. The quantile residual Q-Q plot is from the package DHARMa

```{r glm1-working-in-r}
fit.pois <- glm(Diplo_intensity ~ Treatment, family="poisson", data=worm)

# from the DHARMa package
  n_sim <- 250
  simulationOutput <- simulateResiduals(fittedModel = fit.pois, n = n_sim)
  plot(simulationOutput, asFactor = F)

```

Negative binomial fit.

```{r}
fit.nb <- glm.nb(Diplo_intensity ~ Treatment, data=worm)
# from the DHARMa package
  simulationOutput <- simulateResiduals(fittedModel = fit.nb, n = n_sim)
  plot(simulationOutput, asFactor = F)

# link scale
emm <- emmeans(fit.nb, specs="Treatment")
emm
summary(contrast(emm, method="trt.vs.ctrl", adjust="none"), infer=c(TRUE, TRUE))

emm.response <- emmeans(fit.nb, specs="Treatment", type="response")
summary(contrast(emm, method="trt.vs.ctrl", adjust="none", type="response"), infer=c(TRUE, TRUE))
```

