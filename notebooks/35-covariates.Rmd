---
title: "Covariates"
output: html_notebook
---

# Adding covariates to a linear model I: ANCOVA

```{r setup-ancova, echo=FALSE, warnng=FALSE, message=FALSE}
library(ggplot2)
library(viridis) # color blind palette
library(ggpubr)
library(emmeans)
library(data.table)
library(mvtnorm)

colorblind <- function(n){
  # http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette
  return(c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")[1:n])
}

```

In its most general sense, **Covariates** are simply the $X$ variables in a statistical model. With data from experiments, "covariates" more typically refers to $X$ variables that are added to a model to increase precision of the treatment effects. In observational designs, covariates might be added to a model to 1) increase predictive ability, 2) because the researcher is interested in specific conditional effects, or 3) to eliminate confounding. These are discussed in later chapters.

## Adding covariates can increases the precision of the effect of interest

I use fake data to introduce the concept of **statistical elimination** of a **covariate** in a statistical model. Here I am modeling the effect of a new drug on blood LDL-C levels. LDL is a kind of lipoprotein, which are particles in the blood that transport fats and cholesterol to and from different tissues. LDL-C is cholesterol associated with LDL particles. LDL-C is considered "bad cholesterol" because LDL is believed to transport cholesterol and other lipids to arterial walls, which is the basis for atherosclerosis.

Twenty applied biostats students are recruited and are randomly assigned to either the new or old drug. The response is blood LDL-C level and the two treatment levels are "old" and "new". For the dummy variable coding, I'll make "old" the reference level.

```{r ancova-ldlc, echo=FALSE}
set.seed(7)
n <- 10 # replicates per grop
fat <- rnorm(n*2, mean=.3, sd=.05)
# fat is the fraction of calories from fat in diet.
# think about this parameterization. Most people will vary between 15% and 45%.
drug <- rep(c("old", "new"), each=10) # the treatment assignment
fd <- data.table(drug=factor(drug, factor(c("old", "new"))), fat=fat) # the fake data.table
X <- model.matrix(formula(~ fat + drug), data=fd) # the model matrix

b0 <- 100 # LDL-C for the reference group and zero dietary fat
b1 <- 200 # the effect of dietary fat. Increasing fat by 10% adds
# 200*.1 = 20 mg LDL-C/dL blood to their blood
b2 <- -5 # the effect of the new drug -- lowers LDL-C by 5 mg/dL blood
b <- c(b0, b1, b2) # but the parameters into a vector
ldlc <- X%*%b + rnorm(n, sd=2) # matrix algebra
fd[, ldlc:=ldlc] # add ldlc to the fake data.table

```

Let's model this with

\begin{equation}
ldlc = \beta_0 + \beta_1 drug + \varepsilon
(\#eq:ancova-1)
\end{equation}

where $drug$ is the dummy variable with $old=0$ and $new=1$.

```{r ancova-plot1, echo=FALSE, message=FALSE, fig.cap="The fake LDL-C experiment."}
ggdotplot(x="drug", y="ldlc", data=fd, add="mean_se")
coef(summary(lm(ldlc~drug, data=fd)))
```

The plot shows large overlap in LDL-C and no obvious effect of the drug. There "is no effect of the drug ($p = 0.754$)" is of course an incorrect interpretation of the hypothesis test of the estimate of $\beta_1$. A correct interpretation is, there is too much noise to say much about any effect.

In addition to assigning treatment level randomly, I also had the 20 students count calories from fat over the course of the experiment. Here is a plot of LDL-C vs. percent calories from fat, with treatment assignment color coded. Remember, these are the exact same values of LDL-C as in the first figure.

```{r ancova-plot2, message=FALSE, echo=FALSE, fig.cap="Linear regression of $ldlc$ on dietary $fat$ fit to the fake LDL-C data. The points are color coded by treatment."}
qplot(x=fat, y=ldlc, data=fd) + geom_smooth(method="lm", se=FALSE) + geom_point(aes(color=drug))
```

The line is the bivariate regression fit to the data ignoring treatment level so is the model

\begin{equation}
ldlc = \beta_0 + \beta_1 fat + \varepsilon
(\#eq:ancova-2)
\end{equation}

I've color coded the points by treatment level but $drug$ is not in the model. It is clear that most of the "old" data points are above the line, or have positive residuals from the model, while the "new" data points are below the line, or have negative residuals from the model. A better way to think about this pattern is that **at any specific level of fat, the LDL-C for old is higher than the LDL-C for new**.

What is happening? Dietary fat is contributing to the variance of LDL-C and this added noise makes it harder to measure the effect of the new drug relative to the old drug.
If we could somehow measure the effect of drug at a specific level of dietary fat, then we could get a more precise estimate of the effect. But how to do this?

1. We could just analyze a subset of the data, that is, only the cases in which the value of dietary fat is nearly equal. This throws away perfectly good data and, consequently, greatly reduces the sample size and thus precision to estimate the effect.

2. We could use the residuals of the fitted model \@ref(eq:ancova-2) to estimate the effect of drug treatment (this is what we did by eye in figure \@ref(fig:ancova-plot2)). Here is the new model

\begin{equation}
ldlc.r = \beta_0 + \beta_1 drug + \varepsilon
(\#eq:ancova-3)
\end{equation}

where $ldlc.r$ are the residuals. The effect of the new drug on these residuals is

```{r echo=FALSE}
fit <- lm(ldlc ~ fat, data=fd)
fd[, ldlc.r:=residuals(fit)]
ggdotplot(x="drug", y="ldlc.r", data=fd, add="mean_se")
fit <- lm(ldlc.r~drug, data=fd)
coef(summary(fit))

```

In this two-stage analysis (stage 1: fit ldlc ~ fat to get residuals, stage 2: fit residuals ~ drug), we have *eliminated the effect of dietary fat* on the variance of the response and, as a consequence, the estimate of the effect of the drug is much more precise. Now the estimate of the effect is -4.7 mg/dL blood and the SE is only .9 (compare this to the values in the original analysis). While the SE of the diference is correct, any confidence interval or $t$-value is not because the df is wrong. In the two stage analysis we fit two parameters -- the slope (coefficient $b_1$) in stage 1 and the difference in means (coefficient $b_1$) in stage 2 -- but the $t$ in the table assumes we only fit one parameter (that from stage 2). Effectively, the stage 2 test is ignorant that the data ($ldlc.r$) are the result of a previous model fit. We could manually modify the computation of $t$, but the more proper method is to simply...

3. Add dietary fat into the original linear model.

\begin{equation}
ldlc = \beta_0 + \beta_1 fat + \beta_2 drug + \varepsilon
\end{equation}

```{r echo=FALSE}
fit <- lm(ldlc ~ fat + drug, data=fd)
coef(summary(fit))

```

Here, the estimate is -5.1 and the SE is 0.9. Look back at the script generating the fake data; the true effect ($\beta_1$) of the new drug was set to -5.0 so this estimate is quite good.

### Interaction effects with covariates

### Add only covariates that were measured before peaking at the data

## Regression to the mean

It is common to measure the outcome variable ($Y$) both before and after the experimental treatments are applied and then compare the pre-post *change* in $Y$ in response to the treatment using a $t$-test or ANOVA. Don't do this.

Instead, add the pre-treatment measure into the model as a covariate.

\begin{equation}
Y_{post} = \beta_0 + \beta_1 Y_{pre} + \beta_2 Treatment + \varepsilon
(\#eq:ancova-4)
\end{equation}

where $Treatment$ is a dummy variable for a two-level factor. A pre-treatment measure ($Y_{pre}$) is often called the *baseline* measure. The change in $Y$ ($\Delta Y = Y{post} - Y_{pre}$) is sometimes called a change score or gain score. $\Delta Y$ can be modeled as in equation \@ref(eq:ancova-4) and the $p$-value will be precisely the same (the estimate and SE will differ of course because the response variable is different).

\begin{equation}
\Delta Y = \beta_0 + \beta_1 Y_{pre} + \beta_2 Treatment + \varepsilon
(\#eq:ancova-5)
\end{equation}

The reason why a researcher should not model $\Delta Y$ as a function of $Treatment$ without $Y_{pre}$ as a covariate is that the **regression to the mean**. To explain regression to the mean, I use fake data simulated to model the results from an important study on gut microbiomes. In this study, the authors (Turnbaugh et al. xxx) showed that mice with feces from obese (genotype *ob/ob*) donors had higher weight gain than mice with feces from lean (genotype *+/+*) donors, presumably because of the differences in microbial communities between the donor types (shown elsewhere in their paper). To support the inference of a large difference in weight change, they illustrated the percent change in each treatment level in their Fig 3C, which is replicated here using simulated data generated to match the original summary statistics (Figure \@ref(fig:ancova-mouseplot1)). 

```{r ancova-mouse, echo=FALSE}
# create fake data using available summary statistics from paper. Use a while
# loop to generate a new data set each loop and check how close the summary
# statistics of the fake data are to those of the Turnbaugh data. If all stats
# are within tolerance, keep and plot

# create data.table of Turnbaugh mouse body fat data. Numbers are from paper.
mouse <- data.table(treatment = c('+/+', 'ob/ob'),
                    n = c(10, 9),
                    percent = c(.27, .47),
                    change = c(.86, 1.3),
                    se_percent = c(0.036, 0.083),
                    se_change = c(0.1, 0.2)
                    )
mouse[, init:=change/percent]
mouse[, final:=init+change]
mouse[, sd:=sqrt(se_change^2*n/2)] # sample sd, assume homogenous pre/post
# reorder columns
mouse <- mouse[, .SD, .SDcols=c('treatment', 'n', 'init', 'final', 'sd', 'change', 'percent', 'se_change', 'se_percent')]

# compute some statistics for later use
init_diff <- (mouse[2, init] - mouse[1, init])/sqrt(mean(mouse[,sd^2]))
final_diff <- (mouse[2, final] - mouse[1, final])/sqrt(mean(mouse[,sd^2]))
# generate replica fake data

percent.lean <- mouse[treatment=='+/+', percent]
percent.obese <- mouse[treatment=='ob/ob', percent]
percent.lean.se <- mouse[treatment=='+/+', se_percent]
percent.obese.se <- mouse[treatment=='ob/ob', se_percent]

# parameters for model
Sigma_ii <- mean(mouse[, sd^2]) # variances for both init and final
sigma <- sqrt(Sigma_ii) # standard deviations
rho <- 0.4 # pre-post correlation. I have no idea what the actual value is.
mu <- mean(mouse[, init]) # initial weight
delta <- mean(mouse[, change]) # delta is the post - pre effect for control
tau <- 0 # tau is the treatment effect (on top of delta)
Sigma <- matrix(c(c(Sigma_ii, rho*Sigma_ii), c(rho*Sigma_ii, Sigma_ii)), nrow=2)
N <- sum(mouse[, n])

seed <- 1673 # recovers stats
done <- FALSE
while(done==FALSE){
  set.seed(seed)
  
  # create pre-post mouse weights that are correlated
  treatment <- rep(c('+/+','ob/ob'), mouse[,n])
  weights <- rmvnorm(n=N, mean=c(mu, mu+delta), sigma=Sigma)
  weights[treatment=='ob/ob', 2] <- weights[treatment=='ob/ob', 2] + tau
  fake_mouse <- data.table(ID=factor(1:N),
                   treatment=factor(treatment, c('+/+','ob/ob')),
                   init=weights[,1],
                   final=weights[,2])
  fake_mouse[, change:=final-init]
  fake_mouse[, percent:=change/init]
  fake_mouse_sum <- fake_mouse[, .(final=mean(final),
                   percent_change=mean(percent), 
                   se=sd(percent)/sqrt(.N), 
                   cs=mean(change), 
                   se.change=sd(change)/sqrt(.N)), 
               by=treatment]
  fake_mouse_sum
  if(abs(fake_mouse_sum[treatment=='ob/ob', final] - fake_mouse_sum[treatment=='+/+', final]) < 0.04 &
     abs(fake_mouse_sum[treatment=='ob/ob', percent_change] - percent.obese) < 0.02 &
     abs(fake_mouse_sum[treatment=='+/+', percent_change] - percent.lean) < 0.02 &
     abs(fake_mouse_sum[treatment=='ob/ob', se] - percent.obese.se) < 0.02 &
     abs(fake_mouse_sum[treatment=='+/+', se] - percent.lean.se) < 0.01 ){
    done <- TRUE
  }else{
  seed <- seed+1
  }
}

fake_mouse_sum[, percent_change:=percent_change*100]
fake_mouse_sum[, se:=se*100]
fake_mouse[, percent:=percent*100]
```

```{r ancova-mouseplot1, echo=FALSE, fig.cap="Figure 3c of Turnbaugh *et al* 2006. This figure was generated with simulated data matching the summary statistics given in Turnbaugh *et al* 2006", fig.small = TRUE}
# plot it
gg <- ggplot(data=fake_mouse_sum, aes(x=treatment, y=percent_change, fill=treatment)) +
  geom_errorbar(aes(ymin=(percent_change-se), ymax=(percent_change+se)), width=.2) +
  geom_col(fill=c('white', 'black'), color='black') +
  ylab("Increase in Body Fat (%)") +
  xlab("Donor") +
  scale_y_continuous(limits=c(0,60)) +
  theme_minimal(base_size=18) +
  theme(legend.position='none')
gg

```

That looks like a big difference, with the mice from the obese-donor treatment level gaining much more fat than the mice from the lean-donor treatment level. Turnbaugh et al. used a simple t-test of this percent change to test the effect of the *ob/ob* treatment. The linear model underneath this $t$-test is

\begin{equation}
fat.gain = \beta_0 + \beta_1 obese + \varepsilon
\end{equation}

where $fat.gain$ is the percent gain in fat from baseline and $obese$ is a dummy variable with *ob/ob* $= 1$. The model coefficients are

```{r ancova-mouse-model1, echo=FALSE}
fit1 <- lm(percent ~ treatment, data=fake_mouse)
coef(summary(fit1))
confint(fit1)
```

Or, the increase in fat in the obese-treated mice was 21.9% (95%CI: 4.7, 39.2%, $p=0.016$) greater than the increase in lean-treated mice. This result, if generally verified with replication and rigorous probing, would have spectacular implications for human health.

One might reasonably expect that if mice are randomized into two groups, then the expected difference in percent change from baseline is zero. This is unconditionally true but not conditionally true. That is, if we ignore initial fat weight, the expected difference is zero. But, the expected difference is also conditional on the initial difference in fat weights. More specifically, the expected difference is opposite in sign but proportional in magnitude to the initial difference. This conditional expectation is a consequence of regression to the mean. If the first measure of a random variable is extreme, the second measure will tend to be less extreme. And, if a second measure is extreme, the first measure will tend to be less extreme. 

Despite random treatment assignment, the mean initial fat weight of the *ob/ob* group was 1.2SD less than the mean initial weight of the *+/+* group. By contrast, the mean final weight of the *ob/ob* group was 0.06SD larger than the mean final weight of the *+/+* group. This first difference is an extreme measure. The second is extremely close to the expectation if there is no treatment effect. Because the initial difference in weight is unusually negative, the expected difference in percent change will be unusually positive.

This dependency between difference in percent change from baseline and difference in initial weight is easy to simulate. Simply

1. randomly sample a normal distribution as the "initial weight" and randomly assign to treatment class
2. let the final weight have some correlation ($\rho$) with the initial weight. Some correlation should make sense -- we expect a mouse that has more fat than average at the start of the experiment to also have more fat than average at the end of the experiment. Run the experiment at different values of this correlation to see how it effects regression to the mean.
3. Do not add a treatment effect. We want to explore the behavior of the nill null hypothesis.

```{r ancova-sim1, echo=FALSE, fig.cap="Effect of initial difference in weight on the difference in change score. Increased initial difference in weight results in an increased differences in change score between treatment and control. Four different values of *rho* (the correlation between initial and final weights) were simulated. Only when *rho*=1 is there no influence of initial difference, because whatever differences occur at baseline will be perfectly preserved in the final measure. The X gives the values in the original Turnbaugh data"}
niter <- 1000
n <- 10
N <- n*2
# parameters from code block "originalPlot"
d_obs.init <- mouse[treatment=='ob/ob', init] - mouse[treatment=='+/+', init]
d_obs.change <- mouse[treatment=='ob/ob', change] - mouse[treatment=='+/+', change]
d_obs.percent <- mouse[treatment=='ob/ob', percent] - mouse[treatment=='+/+', percent]

init_diff <- numeric(niter)
cs_diff <- numeric(niter)
percent_diff <- numeric(niter)
res_mat <- data.table(NULL)
for(rho in c(0, 0.33, 0.66, 1)){
  Sigma <- matrix(c(c(sigma^2, rho*sigma^2), c(rho*sigma^2, sigma^2)), nrow=2)
  for(iter in 1:niter){
    # col 1 is initial weights, col 2 is final weights
    weights <- rmvnorm(n=N, mean=c(mu, mu+delta), sigma=Sigma)
    init <- weights[,1]
    final <- weights[,2]
    change <- final - init
    percent <- change/init
    init_diff[iter] <- mean(init[1:n]) - mean(init[(n+1):(n*2)]) 
    cs_diff[iter] <- mean(change[1:n]) - mean(change[(n+1):(n*2)]) 
    percent_diff[iter] <- mean(percent[1:n]) - mean(percent[(n+1):(n*2)]) 
  }
  res_mat <- rbind(res_mat, data.table(rho=rho, init=init_diff, cs=cs_diff, percent=percent_diff))
}

# plot it
res_mat[, rho:=factor(rho)]
labs <- levels(res_mat[, rho])
#cols <- colorblind_brewer(length(labs))
cols <- colorblind(length(labs))
#cols <- viridis(4)[1:4]
gg <- ggplot(data=res_mat, aes(x=init, y=cs, color=rho)) +
  geom_point() +
  geom_point(aes(x=d_obs.init, y=d_obs.change), color='black', shape='X', size=5) +
  ylab("Difference in change score") +
  xlab("Initial difference") +
#  scale_color_viridis(discrete=TRUE) +
  scale_colour_manual(values=cols, labels = labs) +
  theme_minimal(base_size=14) +
  NULL
gg

# modeled p-value
p.value <- sum(abs(res_mat$cs >= d_obs.change))/(niter*4) # 

# conditional p if rho=0.33
res_mat <- rbind(res_mat, data.table(rho=0.33, init=d_obs.init, cs=d_obs.change, percent=d_obs.percent))
pfit <- lm(cs~init, data=res_mat[rho==0.33])
res <- residuals(pfit)
p.cond1 <- sum(abs(res) >= abs(res[1001]))/1001

# conditional p if rho=0.66
res_mat <- rbind(res_mat, data.table(rho=0.66, init=d_obs.init, cs=d_obs.change, percent=d_obs.percent))
pfit <- lm(cs~init, data=res_mat[rho==0.66])
res <- residuals(pfit)
p.cond2 <- sum(abs(res) >= abs(res[1001]))/1001

```

What's happening in Figure \@ref(fig:ancova-sim1)? Each point is a result for a single, simulated experiment. In total, there are 1000 simulated experiments for each of four values of $\rho$. The *x*-axis is the difference between the means of the two treatment levels at baseline (*Initial difference*). The *y*-axis is the difference in mean change score between the two treatment levels -- that is the difference in the means of $\Delta Y$ from equation \@ref(eq:ancova-5). This difference in $\Delta Y$ is the effect of the treatment the researchers are interested in. The *unconditional* expectation of this difference is zero

\begin{equation}
\mathrm{E}(\Delta Y_{ob/ob} - \Delta Y_{+/+}) = 0
\end{equation}

but the change conditional on baseline is not zero

\begin{equation}
\mathrm{E}(\Delta Y_{ob/ob} - \Delta Y_{+/+}) \ne 0
\end{equation}

Instead, the conditional expectation is a function of the difference at baseline. If the initial difference in weight happens to be unusually large and negative, the expected difference in change score is unusually positive. This non-zero expectation means that the estimate of the treatment effect is **conditionally biased** for any model that does not include the baseline fat weight as a covariate. And, from a frequentist perspective, the Type I error for a test of a difference in $\Delta Y$ is strongly dependent on the initial difference in weight.

The big X in the plot indicates the difference at baseline and difference in $\Delta Y$ for the original mice study. The difference in $Delta Y$ is unusually positive (about .6% of the $|\delta Y|$ are larger) but very close to the expected value given the unusually large, negative difference at baseline. In other words, the probability of the data, or more extreme than the data, is not 0.006 but something larger and perhaps, much larger (the computed value depends on the observed $\rho$. From, the plot, the X is very unusual if $\rho=1$, pretty unusual if $\rho=0.66$, but pretty common if $\rho=0.33$ or if $\rho=0$).

### Do not use percent change, believing that percents account for effects of initial weights

Some researchers mistakenly believe that a $t$-test of percent change automatically adjusts for effects in initial weight, since this initial weight is in the denominator of the percent. This is wrong. The dependency of the difference in change between treatments on the initial difference between treatments is more severe if change is measured as a percent, because the numerator (the change score) is expected to be larger if the denominator is smaller (initial measure). Using the simulated data from above, here is this dependency.


```{r ancova-sim2, echo=FALSE, fig.cap="Effect of initial difference in weight on the difference in percent change. Increased initial difference in weight results in an increased differences in Percent change between treatment and control. Four different values of *rho* (the correlation between initial and final weights) were simulated. Note there is no value of *rho* where the difference in percent change is independent of the initial difference. The X gives the values in the original Turnbaugh data."}
gg <- ggplot(data=res_mat, aes(x=init, y=percent, color=rho)) +
  geom_point() +
  geom_point(aes(x=d_obs.init, y=d_obs.percent), color='black', shape='X', size=5) +
  ylab("Difference in percent change") +
  xlab("Initial difference") +
  scale_colour_manual(values=cols, labels = labs) +
  theme_minimal(base_size=14) +
  NULL
gg

```


### Do not "test for balance" of baseline measures

Contrary to some advice and maybe to intuition, it makes no sense to "test for balance" at baseline with a *t*-test of the difference in initial measures of $Y$. And, it makes no sense to use this test as a decision rule for how to proceed: if $p>0.05$ then use a simple $t$ test of the change scores, if $p<0.05$ then use ANCOVA with baseline measures in the model. First, a null-hypothesis significance test cannot tell you that there is "no difference" -- this is not what null-hypothesis tests do. Second, any $p$-value after the initial test isn't strictly valid as it does not take into account this decision step, but this is minor. Third, it doesn't matter; there will always be some difference in the actual means of the initial measures and, consequently, the conditional expectation of the final measures, or change in measures, or percent change will be dependent on this initial difference. So, if one has initial measures, one should use an linear model that adjusts for baseline measures to estimate the treatment effect in pre-post designs. And, if one isn't planning on taking an initial measure, then maybe you should, because the initial measure used in a linear model allows a better estimate of the treatment effect!

