---
title: "22-Model Assumptions"
output: html_notebook
---

# Linear Model Assumptions

The linear model

\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i
\end{equation}

has several important assumptions for the computation of correct standard errors and any statistic derived from standard errors including a confidence interval and a $p$-value.

1. Normality: the errors $\varepsilon$ have mean zero and a normal distribution. Often this is mistakenly interpreted as the response $Y$ has to be normally distributed and sometimes that even $X$ has to be normally distributed. But the assumption applies only to the residuals of the fit model.

2. Independence: the errors are independent of each other

3. Homoskedasticity: the variance of the errors is homogenous

3. the errors are identically distributed

4. the error is independent of any $X$

5. the error is independent of the response

Here I model fake data in which $X$ is niformly distributed. $Y$ is a linear function of $X$ + normally distributed error, so the $Y$ is also uniformly distributed but the error is normal. The histogram of $Y$ shows this uniform distribution clearly. The histogram of the residuals from the fit model show the normal distribution of the error clearly.
d
```{r}
n <- 1000
x <- runif(n)*100
sigma <- 1.0
y <- 5 + 1.2*x + rnorm(n, sd=sigma)
qplot(y)

fit <- lm(y ~ x)
summary(fit)
qplot(residuals(fit))
```

There are tests for normality but I do not recommend these because the test doesn't add any value from what one can gain by simply inspecting the residuals. 

```{r}
n <- 30
h <- runif(n)*90 + 10
w <- h^3
sigma <- rnorm(n)*.2
logh <- log(h)
logw <- log(w) + sigma
qplot(logh, logw)

x <- h
y <- exp(logw)
qplot(x=x, y=y)
fit <- lm(y ~ x)
qplot(residuals(fit))

plot(fit)

```

